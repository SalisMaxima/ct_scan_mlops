{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CT Scan MLOps","text":"<p>Chest CT scan multi-classification model for lung tumor detection using PyTorch Lightning.</p>"},{"location":"#project-goal","title":"Project Goal","text":"<p>This project builds an image multi-classification model that detects whether a chest CT scan shows signs of three different types of lung tumor or is normal:</p> <ol> <li>Adenocarcinoma (left lower lobe)</li> <li>Large cell carcinoma (left hilum)</li> <li>Squamous cell carcinoma (left hilum)</li> <li>Normal</li> </ol>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>PyTorch Lightning for structured, scalable training</li> <li>Hydra for flexible configuration management</li> <li>Weights &amp; Biases for experiment tracking and team collaboration</li> <li>DVC for data versioning with GCS backend</li> <li>FastAPI for model inference API</li> <li>Docker support for containerized training and deployment</li> <li>Comprehensive testing with pytest</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Clone and setup\ngit clone https://github.com/SalisMaxima/ct_scan_mlops.git\ncd ct_scan_mlops\nuv venv &amp;&amp; source .venv/bin/activate\nuv sync --all-groups\n\n# Pull data and train\ndvc pull\ninvoke train\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>The project includes two model architectures:</p> Model Description CustomCNN Configurable CNN baseline for comparison ResNet18 Transfer learning with pretrained ImageNet weights"},{"location":"#dataset","title":"Dataset","text":"<p>Source: Chest CT-Scan Images Dataset from Kaggle</p> <p>The dataset contains ~1000 images with balanced volume across 4 classifications. Data augmentation with rotations and translations is applied to improve model robustness.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"-   :material-download: **[Getting Started](getting-started/index.md)**      ---      Installation, setup, and first steps  -   :material-book-open: **[User Guide](user-guide/index.md)**      ---      Training, evaluation, and configuration  -   :material-api: **[API Reference](api-reference/index.md)**      ---      Module and class documentation  -   :material-wrench: **[Development](development/index.md)**      ---      Contributing and project structure"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub Repository</li> <li>W&amp;B Dashboard</li> <li>Kaggle Dataset</li> <li>DTU MLOps Course</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Auto-generated documentation from source code docstrings.</p>"},{"location":"api-reference/#modules","title":"Modules","text":"Module Description Data Data loading and preprocessing Model Neural network architectures Training Training pipeline and Lightning module Evaluation Model evaluation and metrics FastAPI Inference API endpoints Utilities Shared utility functions"},{"location":"api-reference/#quick-links","title":"Quick Links","text":""},{"location":"api-reference/#data-module","title":"Data Module","text":"<ul> <li><code>ChestCTDataset</code> - Raw image dataset</li> <li><code>ProcessedChestCTDataset</code> - Preprocessed tensor dataset</li> <li><code>ChestCTDataModule</code> - Lightning DataModule</li> <li><code>preprocess()</code> - Preprocess raw images to tensors</li> </ul>"},{"location":"api-reference/#model-module","title":"Model Module","text":"<ul> <li><code>CustomCNN</code> - Configurable CNN</li> <li><code>ResNet18</code> - Transfer learning model</li> <li><code>build_model()</code> - Build model from config</li> </ul>"},{"location":"api-reference/#training-module","title":"Training Module","text":"<ul> <li><code>LitModel</code> - Lightning module wrapper</li> <li><code>train_model()</code> - Main training function</li> </ul>"},{"location":"api-reference/#evaluation-module","title":"Evaluation Module","text":"<ul> <li><code>evaluate_model()</code> - Evaluate a model</li> <li><code>load_model_from_checkpoint()</code> - Load model from checkpoint</li> </ul>"},{"location":"api-reference/api/","title":"FastAPI Module","text":"<p>REST API for CT scan classification inference.</p>"},{"location":"api-reference/api/#overview","title":"Overview","text":"<p>The API module provides:</p> <ul> <li>FastAPI application for serving predictions</li> <li>Health check endpoint for monitoring</li> <li>Prediction endpoint for classification</li> <li>Feedback endpoint for collecting corrections</li> <li>Prometheus metrics for observability</li> </ul>"},{"location":"api-reference/api/#application","title":"Application","text":"<p>The FastAPI application is initialized with a lifespan manager that loads the model on startup.</p>"},{"location":"api-reference/api/#endpoints","title":"Endpoints","text":"Endpoint Method Description <code>/health</code> GET Health check and status <code>/predict</code> POST Classify a CT scan image <code>/feedback</code> POST Submit prediction feedback <code>/metrics</code> GET Prometheus metrics"},{"location":"api-reference/api/#api-reference","title":"API Reference","text":""},{"location":"api-reference/api/#health-check","title":"Health Check","text":"<pre><code>GET /health\n\nResponse:\n{\n    \"ok\": true,\n    \"device\": \"cuda\",\n    \"model_loaded\": true,\n    \"config_path\": \"configs/config.yaml\",\n    \"model_path\": \"models/model.pt\"\n}\n</code></pre>"},{"location":"api-reference/api/#prediction","title":"Prediction","text":"<pre><code>POST /predict\nContent-Type: multipart/form-data\nfile: &lt;image file&gt;\n\nResponse:\n{\n    \"pred_index\": 0,\n    \"pred_class\": \"adenocarcinoma\"\n}\n</code></pre>"},{"location":"api-reference/api/#feedback","title":"Feedback","text":"<pre><code>POST /feedback\nContent-Type: multipart/form-data\nfile: &lt;image file&gt;\npredicted_class: \"adenocarcinoma\"\nis_correct: false\ncorrect_class: \"normal\"\n\nResponse:\n{\n    \"saved_to\": \"feedback/normal/image-abc123.png\",\n    \"class\": \"normal\"\n}\n</code></pre>"},{"location":"api-reference/api/#configuration","title":"Configuration","text":"<p>Configure the API with environment variables:</p> Variable Description Default <code>CONFIG_PATH</code> Path to Hydra config <code>configs/config.yaml</code> <code>MODEL_PATH</code> Path to model weights <code>models/model.pt</code> <code>LOAD_MODEL</code> Load model on startup (<code>1</code>/<code>0</code>) <code>1</code> <code>FEEDBACK_DIR</code> Directory for feedback images <code>feedback</code> <code>DRIFT_CURRENT_PATH</code> Path for drift detection CSV <code>data/drift/current.csv</code>"},{"location":"api-reference/api/#monitoring","title":"Monitoring","text":""},{"location":"api-reference/api/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>The API exposes Prometheus metrics at <code>/metrics</code>:</p> <ul> <li><code>prediction_error</code> - Counter of prediction errors</li> <li><code>system_cpu_percent</code> - System CPU utilization</li> <li><code>system_memory_percent</code> - System memory utilization</li> <li><code>process_rss_bytes</code> - Process memory usage</li> <li>HTTP request metrics (via prometheus-fastapi-instrumentator)</li> </ul>"},{"location":"api-reference/api/#data-drift-tracking","title":"Data Drift Tracking","text":"<p>Each prediction appends image statistics to a CSV file for drift detection:</p> <ul> <li>Mean, std, min, max of pixel values</li> <li>Percentiles (1st, 50th, 99th)</li> <li>Image dimensions</li> <li>Prediction class and confidence</li> </ul>"},{"location":"api-reference/api/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/api/#run-the-api","title":"Run the API","text":"<pre><code># With invoke\ninvoke api\n\n# Direct with uvicorn\nuvicorn ct_scan_mlops.api:app --host 0.0.0.0 --port 8000\n\n# With custom config\nCONFIG_PATH=custom/config.yaml MODEL_PATH=models/best.pt uvicorn ct_scan_mlops.api:app\n</code></pre>"},{"location":"api-reference/api/#make-predictions","title":"Make Predictions","text":"<pre><code># Using curl\ncurl -X POST \"http://localhost:8000/predict\" \\\n    -F \"file=@path/to/ct_scan.png\"\n\n# Using Python requests\nimport requests\n\nwith open(\"ct_scan.png\", \"rb\") as f:\n    response = requests.post(\n        \"http://localhost:8000/predict\",\n        files={\"file\": f}\n    )\n\nprint(response.json())\n# {\"pred_index\": 0, \"pred_class\": \"adenocarcinoma\"}\n</code></pre>"},{"location":"api-reference/api/#submit-feedback","title":"Submit Feedback","text":"<pre><code>curl -X POST \"http://localhost:8000/feedback\" \\\n    -F \"file=@ct_scan.png\" \\\n    -F \"predicted_class=adenocarcinoma\" \\\n    -F \"is_correct=false\" \\\n    -F \"correct_class=normal\"\n</code></pre>"},{"location":"api-reference/api/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Build\ndocker build -f dockerfiles/api.dockerfile -t ct-scan-api .\n\n# Run\ndocker run -p 8000:8000 \\\n    -v $(pwd)/models:/app/models \\\n    -v $(pwd)/configs:/app/configs \\\n    ct-scan-api\n</code></pre>"},{"location":"api-reference/data/","title":"Data Module","text":"<p>Data loading and preprocessing for CT scan images.</p>"},{"location":"api-reference/data/#overview","title":"Overview","text":"<p>The data module provides:</p> <ul> <li>Dataset classes for loading raw images or preprocessed tensors</li> <li>Lightning DataModule for integration with PyTorch Lightning</li> <li>Preprocessing functions to convert raw images to tensors</li> <li>Data augmentation with Albumentations</li> </ul>"},{"location":"api-reference/data/#constants","title":"Constants","text":"<pre><code># Canonical class labels\nCLASSES = [\n    \"adenocarcinoma\",\n    \"large_cell_carcinoma\",\n    \"squamous_cell_carcinoma\",\n    \"normal\",\n]\n\n# ImageNet normalization stats (default)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n</code></pre>"},{"location":"api-reference/data/#dataset-classes","title":"Dataset Classes","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataset","title":"ChestCTDataset","text":"<pre><code>ChestCTDataset(\n    data_dir: str | Path,\n    split: str = \"train\",\n    transform: Compose | None = None,\n    image_size: int = 224,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for Chest CT scan images (loads from raw images).</p> <p>Loads images from disk and applies transforms on-the-fly. Use ProcessedChestCTDataset for faster loading with preprocessed tensors.</p> <p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str | Path</code> <p>Path to data root (containing train/valid/test folders)</p> required <code>split</code> <code>str</code> <p>One of 'train', 'valid', 'test'</p> <code>'train'</code> <code>transform</code> <code>Compose | None</code> <p>Albumentations transform pipeline</p> <code>None</code> <code>image_size</code> <code>int</code> <p>Target image size (used if transform is None)</p> <code>224</code> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str | Path,\n    split: str = \"train\",\n    transform: A.Compose | None = None,\n    image_size: int = 224,\n) -&gt; None:\n    \"\"\"Initialize dataset.\n\n    Args:\n        data_dir: Path to data root (containing train/valid/test folders)\n        split: One of 'train', 'valid', 'test'\n        transform: Albumentations transform pipeline\n        image_size: Target image size (used if transform is None)\n    \"\"\"\n    if split not in {\"train\", \"valid\", \"test\"}:\n        raise ValueError(\"split must be one of: 'train', 'valid', 'test'\")\n\n    self.data_dir = Path(data_dir)\n    self.split = split\n    self.split_dir = self.data_dir / split\n\n    if not self.split_dir.exists():\n        raise FileNotFoundError(f\"Split directory not found: {self.split_dir}\")\n\n    # Class mapping\n    self.class_to_idx = {c: i for i, c in enumerate(CLASSES)}\n    self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n    self.num_classes = len(CLASSES)\n\n    # Transform pipeline\n    if transform is not None:\n        self.transform = transform\n    else:\n        self.transform = get_transforms(split, image_size=image_size)\n\n    # Collect samples\n    self.samples: list[tuple[Path, int]] = []\n    for class_folder in sorted(p for p in self.split_dir.iterdir() if p.is_dir()):\n        try:\n            label = _infer_label_from_folder(class_folder.name, self.class_to_idx)\n        except ValueError as e:\n            logger.warning(f\"Skipping folder: {e}\")\n            continue\n\n        for img_path in class_folder.rglob(\"*\"):\n            if img_path.is_file() and img_path.suffix.lower() in IMG_EXTS:\n                self.samples.append((img_path, label))\n\n    if len(self.samples) == 0:\n        raise FileNotFoundError(f\"No images found in {self.split_dir}\")\n\n    logger.info(f\"Loaded {len(self.samples)} images for split '{split}'\")\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ProcessedChestCTDataset","title":"ProcessedChestCTDataset","text":"<pre><code>ProcessedChestCTDataset(\n    data_path: Path, split: str = \"train\"\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for preprocessed Chest CT scan images (loads from .pt files).</p> <p>This is the fast dataset class that loads pre-processed tensors from disk. Use preprocess() to generate the .pt files first.</p> <p>Initialize dataset from processed tensors.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to processed data folder (containing .pt files)</p> required <code>split</code> <code>str</code> <p>One of 'train', 'valid', 'test'</p> <code>'train'</code> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def __init__(self, data_path: Path, split: str = \"train\") -&gt; None:\n    \"\"\"Initialize dataset from processed tensors.\n\n    Args:\n        data_path: Path to processed data folder (containing .pt files)\n        split: One of 'train', 'valid', 'test'\n    \"\"\"\n    self.data_path = Path(data_path)\n    self.split = split\n\n    # Load pre-processed tensors\n    images_path = self.data_path / f\"{split}_images.pt\"\n    labels_path = self.data_path / f\"{split}_labels.pt\"\n\n    if not images_path.exists():\n        raise FileNotFoundError(\n            f\"Processed data not found at {images_path}. \"\n            f\"Run 'invoke preprocess-data' or 'python -m ct_scan_mlops.data' first.\"\n        )\n\n    self.images = torch.load(images_path, weights_only=True)\n    self.labels = torch.load(labels_path, weights_only=True)\n\n    # Class mapping\n    self.class_to_idx = {c: i for i, c in enumerate(CLASSES)}\n    self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n    self.num_classes = len(CLASSES)\n\n    logger.info(f\"Loaded {len(self.labels)} processed samples for split '{split}'\")\n</code></pre>"},{"location":"api-reference/data/#lightning-datamodule","title":"Lightning DataModule","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule","title":"ChestCTDataModule","text":"<pre><code>ChestCTDataModule(\n    cfg: DictConfig, use_processed: bool = True\n)\n</code></pre> <p>               Bases: <code>LightningDataModule</code></p> <p>Lightning DataModule for Chest CT scan dataset.</p> <p>Provides a reusable, self-contained data handling structure following PyTorch Lightning best practices.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration containing data and paths settings</p> required <code>use_processed</code> <code>bool</code> <p>If True, use processed tensors (faster). If False, load raw images.</p> <code>True</code> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def __init__(self, cfg: DictConfig, use_processed: bool = True) -&gt; None:\n    super().__init__()\n    self.cfg = cfg\n    self.use_processed = use_processed\n    self.data_cfg = cfg.data\n\n    self.train_ds: Dataset | None = None\n    self.val_ds: Dataset | None = None\n    self.test_ds: Dataset | None = None\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule.predict_dataloader","title":"predict_dataloader","text":"<pre><code>predict_dataloader() -&gt; DataLoader\n</code></pre> <p>Create prediction dataloader (uses test set by default).</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n    \"\"\"Create prediction dataloader (uses test set by default).\"\"\"\n    if self.test_ds is None:\n        self.setup(stage=\"test\")\n    return self.test_dataloader()\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule.setup","title":"setup","text":"<pre><code>setup(stage: str | None = None) -&gt; None\n</code></pre> <p>Load datasets for the specified stage.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>One of 'fit', 'validate', 'test', or 'predict'</p> <code>None</code> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"Load datasets for the specified stage.\n\n    Args:\n        stage: One of 'fit', 'validate', 'test', or 'predict'\n    \"\"\"\n    processed_path = Path(self.data_cfg.get(\"processed_path\", \"data/processed\"))\n\n    # Check if processed data exists\n    if self.use_processed and (processed_path / \"train_images.pt\").exists():\n        logger.info(f\"Using preprocessed data from {processed_path}\")\n        if stage == \"fit\" or stage is None:\n            self.train_ds = ProcessedChestCTDataset(processed_path, split=\"train\")\n            self.val_ds = ProcessedChestCTDataset(processed_path, split=\"valid\")\n        if stage == \"test\" or stage is None:\n            self.test_ds = ProcessedChestCTDataset(processed_path, split=\"test\")\n    else:\n        if self.use_processed:\n            logger.warning(\"Processed data not found, falling back to raw images\")\n        logger.info(\"Loading raw images (slower)\")\n\n        raw_cfg_path = Path(self.cfg.paths.data_dir)\n        if not raw_cfg_path.exists():\n            raise FileNotFoundError(\n                \"Raw dataset path does not exist: \"\n                f\"{raw_cfg_path}.\\n\"\n                \"Fix by pulling/downloading data and (optionally) preprocessing:\\n\"\n                \"  - invoke dvc-pull\\n\"\n                \"  - invoke preprocess-data\\n\"\n                \"Or update configs/config.yaml -&gt; paths.data_dir to point at your dataset root.\"\n            )\n\n        data_dir = _find_data_root(Path(self.cfg.paths.data_dir))\n\n        # Get normalization stats from config\n        mean = list(self.data_cfg.normalize.mean)\n        std = list(self.data_cfg.normalize.std)\n        image_size = self.data_cfg.image_size\n\n        # Create transforms for each split\n        train_transform = get_transforms(\n            \"train\",\n            image_size=image_size,\n            mean=mean,\n            std=std,\n            augmentation_cfg=self.data_cfg.augmentation,\n        )\n        eval_transform = get_transforms(\n            \"valid\",\n            image_size=image_size,\n            mean=mean,\n            std=std,\n        )\n\n        if stage == \"fit\" or stage is None:\n            self.train_ds = ChestCTDataset(data_dir, split=\"train\", transform=train_transform)\n            self.val_ds = ChestCTDataset(data_dir, split=\"valid\", transform=eval_transform)\n        if stage == \"test\" or stage is None:\n            self.test_ds = ChestCTDataset(data_dir, split=\"test\", transform=eval_transform)\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule.test_dataloader","title":"test_dataloader","text":"<pre><code>test_dataloader() -&gt; DataLoader\n</code></pre> <p>Create test dataloader.</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n    \"\"\"Create test dataloader.\"\"\"\n    return DataLoader(\n        self.test_ds,\n        batch_size=self.data_cfg.batch_size,\n        shuffle=False,\n        num_workers=self.data_cfg.num_workers,\n        pin_memory=self.data_cfg.get(\"pin_memory\", True),\n        persistent_workers=self.data_cfg.get(\"persistent_workers\", False) and self.data_cfg.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule.train_dataloader","title":"train_dataloader","text":"<pre><code>train_dataloader() -&gt; DataLoader\n</code></pre> <p>Create training dataloader.</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"Create training dataloader.\"\"\"\n    return DataLoader(\n        self.train_ds,\n        batch_size=self.data_cfg.batch_size,\n        shuffle=True,\n        num_workers=self.data_cfg.num_workers,\n        pin_memory=self.data_cfg.get(\"pin_memory\", True),\n        persistent_workers=self.data_cfg.get(\"persistent_workers\", False) and self.data_cfg.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.ChestCTDataModule.val_dataloader","title":"val_dataloader","text":"<pre><code>val_dataloader() -&gt; DataLoader\n</code></pre> <p>Create validation dataloader.</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n    \"\"\"Create validation dataloader.\"\"\"\n    return DataLoader(\n        self.val_ds,\n        batch_size=self.data_cfg.batch_size,\n        shuffle=False,\n        num_workers=self.data_cfg.num_workers,\n        pin_memory=self.data_cfg.get(\"pin_memory\", True),\n        persistent_workers=self.data_cfg.get(\"persistent_workers\", False) and self.data_cfg.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"api-reference/data/#functions","title":"Functions","text":""},{"location":"api-reference/data/#data-download","title":"Data Download","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.download_data","title":"download_data","text":"<pre><code>download_data(target_dir: str | Path = 'data/raw') -&gt; Path\n</code></pre> <p>Download CT scan dataset from Kaggle.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>str | Path</code> <p>Directory to save the downloaded data</p> <code>'data/raw'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the downloaded data directory</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If kagglehub is not installed</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def download_data(target_dir: str | Path = \"data/raw\") -&gt; Path:\n    \"\"\"Download CT scan dataset from Kaggle.\n\n    Args:\n        target_dir: Directory to save the downloaded data\n\n    Returns:\n        Path to the downloaded data directory\n\n    Raises:\n        ImportError: If kagglehub is not installed\n    \"\"\"\n    try:\n        import kagglehub\n    except ImportError as e:\n        raise ImportError(\n            \"kagglehub is required for downloading data. Install it with: pip install kagglehub or uv add kagglehub\"\n        ) from e\n\n    target_path = Path(target_dir)\n    target_path.mkdir(parents=True, exist_ok=True)\n\n    print(\"Downloading dataset from Kaggle...\")\n    cache_path = kagglehub.dataset_download(\"mohamedhanyyy/chest-ctscan-images\")\n\n    # Copy to target directory\n    final_path = target_path / \"chest-ctscan-images\"\n    if final_path.exists():\n        print(f\"Removing existing data at {final_path}...\")\n        shutil.rmtree(final_path)\n\n    print(f\"Copying data to {final_path}...\")\n    shutil.copytree(cache_path, final_path)\n\n    print(f\"\u2713 Dataset downloaded successfully to: {final_path}\")\n    return final_path\n</code></pre>"},{"location":"api-reference/data/#preprocessing","title":"Preprocessing","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.preprocess","title":"preprocess","text":"<pre><code>preprocess(\n    raw_dir: str | Path = \"data/raw\",\n    output_dir: str | Path = \"data/processed\",\n    image_size: int = 224,\n) -&gt; dict[str, Any]\n</code></pre> <p>Preprocess raw images and save as tensors.</p> <p>Loads all images, resizes them, normalizes to mean=0/std=1, and saves as PyTorch tensors for fast loading during training.</p> <p>Parameters:</p> Name Type Description Default <code>raw_dir</code> <code>str | Path</code> <p>Path to raw data directory</p> <code>'data/raw'</code> <code>output_dir</code> <code>str | Path</code> <p>Path to save processed data</p> <code>'data/processed'</code> <code>image_size</code> <code>int</code> <p>Target image size</p> <code>224</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with preprocessing statistics</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def preprocess(\n    raw_dir: str | Path = \"data/raw\",\n    output_dir: str | Path = \"data/processed\",\n    image_size: int = 224,\n) -&gt; dict[str, Any]:\n    \"\"\"Preprocess raw images and save as tensors.\n\n    Loads all images, resizes them, normalizes to mean=0/std=1,\n    and saves as PyTorch tensors for fast loading during training.\n\n    Args:\n        raw_dir: Path to raw data directory\n        output_dir: Path to save processed data\n        image_size: Target image size\n\n    Returns:\n        Dictionary with preprocessing statistics\n    \"\"\"\n    print(f\"Preprocessing data from {raw_dir}...\")\n\n    data_root = _find_data_root(Path(raw_dir))\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    class_to_idx = {c: i for i, c in enumerate(CLASSES)}\n    stats: dict[str, Any] = {\"image_size\": image_size, \"classes\": CLASSES}\n\n    # Simple transform for preprocessing (resize only, no augmentation)\n    preprocess_transform = A.Compose(\n        [\n            A.Resize(image_size, image_size),\n            ToTensorV2(),\n        ]\n    )\n\n    for split in [\"train\", \"valid\", \"test\"]:\n        split_dir = data_root / split\n        if not split_dir.exists():\n            print(f\"  Warning: Split '{split}' not found, skipping\")\n            continue\n\n        images_list: list[torch.Tensor] = []\n        labels_list: list[int] = []\n\n        # Collect all image paths\n        all_samples: list[tuple[Path, int]] = []\n        for class_folder in sorted(p for p in split_dir.iterdir() if p.is_dir()):\n            try:\n                label = _infer_label_from_folder(class_folder.name, class_to_idx)\n            except ValueError:\n                continue\n\n            for img_path in class_folder.rglob(\"*\"):\n                if img_path.is_file() and img_path.suffix.lower() in IMG_EXTS:\n                    all_samples.append((img_path, label))\n\n        print(f\"  Processing {len(all_samples)} images for split '{split}'...\")\n\n        for img_path, label in tqdm(all_samples, desc=f\"  {split}\"):\n            img = Image.open(img_path).convert(\"RGB\")\n            img_array = np.array(img)\n            transformed = preprocess_transform(image=img_array)\n            images_list.append(transformed[\"image\"])\n            labels_list.append(label)\n\n        # Stack into tensors\n        images = torch.stack(images_list).float() / 255.0  # Scale to [0, 1]\n        labels = torch.tensor(labels_list, dtype=torch.long)\n\n        # Normalize to mean=0, std=1 (computed per-split for train, applied same way)\n        if split == \"train\":\n            # Compute and save normalization stats from training data\n            mean = images.mean(dim=(0, 2, 3)).tolist()\n            std = images.std(dim=(0, 2, 3)).tolist()\n            stats[\"mean\"] = mean\n            stats[\"std\"] = std\n            print(\n                f\"  Computed normalization stats - mean: {[f'{m:.4f}' for m in mean]}, std: {[f'{s:.4f}' for s in std]}\"\n            )\n\n        # Apply normalization\n        images = normalize(images)\n\n        # Save tensors\n        torch.save(images, output_path / f\"{split}_images.pt\")\n        torch.save(labels, output_path / f\"{split}_labels.pt\")\n\n        stats[f\"{split}_count\"] = len(labels)\n        stats[f\"{split}_shape\"] = list(images.shape)\n        print(f\"  Saved {split}: {images.shape}\")\n\n    # Save stats\n    torch.save(stats, output_path / \"stats.pt\")\n\n    print(\"\\nPreprocessing complete!\")\n    print(f\"  Output directory: {output_path}\")\n    print(f\"  Train samples: {stats.get('train_count', 0)}\")\n    print(f\"  Valid samples: {stats.get('valid_count', 0)}\")\n    print(f\"  Test samples: {stats.get('test_count', 0)}\")\n\n    return stats\n</code></pre>"},{"location":"api-reference/data/#transform-creation","title":"Transform Creation","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.get_transforms","title":"get_transforms","text":"<pre><code>get_transforms(\n    split: str,\n    image_size: int = 224,\n    mean: list[float] | None = None,\n    std: list[float] | None = None,\n    augmentation_cfg: DictConfig | None = None,\n) -&gt; A.Compose\n</code></pre> <p>Get Albumentations transforms for a given split.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>One of 'train', 'valid', 'test'</p> required <code>image_size</code> <code>int</code> <p>Target image size</p> <code>224</code> <code>mean</code> <code>list[float] | None</code> <p>Normalization mean (default: ImageNet)</p> <code>None</code> <code>std</code> <code>list[float] | None</code> <p>Normalization std (default: ImageNet)</p> <code>None</code> <code>augmentation_cfg</code> <code>DictConfig | None</code> <p>Hydra config for augmentation settings</p> <code>None</code> <p>Returns:</p> Type Description <code>Compose</code> <p>Albumentations Compose pipeline</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def get_transforms(\n    split: str,\n    image_size: int = 224,\n    mean: list[float] | None = None,\n    std: list[float] | None = None,\n    augmentation_cfg: DictConfig | None = None,\n) -&gt; A.Compose:\n    \"\"\"Get Albumentations transforms for a given split.\n\n    Args:\n        split: One of 'train', 'valid', 'test'\n        image_size: Target image size\n        mean: Normalization mean (default: ImageNet)\n        std: Normalization std (default: ImageNet)\n        augmentation_cfg: Hydra config for augmentation settings\n\n    Returns:\n        Albumentations Compose pipeline\n    \"\"\"\n    if mean is None:\n        mean = IMAGENET_MEAN\n    if std is None:\n        std = IMAGENET_STD\n\n    # Base transforms (always applied)\n    base_transforms = [\n        A.Resize(image_size, image_size),\n        A.Normalize(mean=mean, std=std),\n        ToTensorV2(),\n    ]\n\n    if split == \"train\" and augmentation_cfg is not None:\n        train_cfg = augmentation_cfg.get(\"train\", {})\n        aug_transforms = []\n\n        if train_cfg.get(\"horizontal_flip\", False):\n            aug_transforms.append(A.HorizontalFlip(p=0.5))\n\n        if train_cfg.get(\"vertical_flip\", False):\n            aug_transforms.append(A.VerticalFlip(p=0.5))\n\n        rotation_limit = train_cfg.get(\"rotation_limit\", 0)\n        if rotation_limit &gt; 0:\n            aug_transforms.append(A.Rotate(limit=rotation_limit, p=0.5))\n\n        brightness = train_cfg.get(\"brightness_limit\", 0)\n        contrast = train_cfg.get(\"contrast_limit\", 0)\n        if brightness &gt; 0 or contrast &gt; 0:\n            aug_transforms.append(\n                A.RandomBrightnessContrast(\n                    brightness_limit=brightness,\n                    contrast_limit=contrast,\n                    p=0.5,\n                )\n            )\n\n        # Augmentations first, then resize + normalize\n        return A.Compose(aug_transforms + base_transforms)\n\n    # Validation/test: only resize and normalize\n    return A.Compose(base_transforms)\n</code></pre>"},{"location":"api-reference/data/#dataloader-creation","title":"DataLoader Creation","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.create_dataloaders","title":"create_dataloaders","text":"<pre><code>create_dataloaders(\n    cfg: DictConfig, use_processed: bool = True\n) -&gt; tuple[\n    DataLoader[Any], DataLoader[Any], DataLoader[Any]\n]\n</code></pre> <p>Create train, validation, and test dataloaders from config.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config containing data and paths settings</p> required <code>use_processed</code> <code>bool</code> <p>If True, use processed tensors (faster). If False, load raw images.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[DataLoader[Any], DataLoader[Any], DataLoader[Any]]</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def create_dataloaders(\n    cfg: DictConfig,\n    use_processed: bool = True,\n) -&gt; tuple[DataLoader[Any], DataLoader[Any], DataLoader[Any]]:\n    \"\"\"Create train, validation, and test dataloaders from config.\n\n    Args:\n        cfg: Hydra config containing data and paths settings\n        use_processed: If True, use processed tensors (faster). If False, load raw images.\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n    \"\"\"\n    data_cfg = cfg.data\n    processed_path = Path(data_cfg.get(\"processed_path\", \"data/processed\"))\n\n    # Check if processed data exists\n    if use_processed and (processed_path / \"train_images.pt\").exists():\n        logger.info(f\"Using preprocessed data from {processed_path}\")\n        train_ds = ProcessedChestCTDataset(processed_path, split=\"train\")\n        val_ds = ProcessedChestCTDataset(processed_path, split=\"valid\")\n        test_ds = ProcessedChestCTDataset(processed_path, split=\"test\")\n    else:\n        if use_processed:\n            logger.warning(\"Processed data not found, falling back to raw images\")\n        logger.info(\"Loading raw images (slower)\")\n\n        data_dir = _find_data_root(Path(cfg.paths.data_dir))\n\n        # Get normalization stats from config\n        mean = list(data_cfg.normalize.mean)\n        std = list(data_cfg.normalize.std)\n        image_size = data_cfg.image_size\n\n        # Create transforms for each split\n        train_transform = get_transforms(\n            \"train\",\n            image_size=image_size,\n            mean=mean,\n            std=std,\n            augmentation_cfg=data_cfg.augmentation,\n        )\n        eval_transform = get_transforms(\n            \"valid\",\n            image_size=image_size,\n            mean=mean,\n            std=std,\n        )\n\n        train_ds = ChestCTDataset(data_dir, split=\"train\", transform=train_transform)\n        val_ds = ChestCTDataset(data_dir, split=\"valid\", transform=eval_transform)\n        test_ds = ChestCTDataset(data_dir, split=\"test\", transform=eval_transform)\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=data_cfg.batch_size,\n        shuffle=True,\n        num_workers=data_cfg.num_workers,\n        pin_memory=data_cfg.get(\"pin_memory\", True),\n        persistent_workers=data_cfg.get(\"persistent_workers\", False) and data_cfg.num_workers &gt; 0,\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=data_cfg.batch_size,\n        shuffle=False,\n        num_workers=data_cfg.num_workers,\n        pin_memory=data_cfg.get(\"pin_memory\", True),\n        persistent_workers=data_cfg.get(\"persistent_workers\", False) and data_cfg.num_workers &gt; 0,\n    )\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=data_cfg.batch_size,\n        shuffle=False,\n        num_workers=data_cfg.num_workers,\n        pin_memory=data_cfg.get(\"pin_memory\", True),\n        persistent_workers=data_cfg.get(\"persistent_workers\", False) and data_cfg.num_workers &gt; 0,\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api-reference/data/#convenience-helpers","title":"Convenience Helpers","text":""},{"location":"api-reference/data/#ct_scan_mlops.data.chest_ct","title":"chest_ct","text":"<pre><code>chest_ct(\n    raw_dir: str | Path = \"data/raw\", image_size: int = 224\n) -&gt; tuple[Dataset, Dataset, Dataset]\n</code></pre> <p>Convenience helper to load train, val, and test datasets from raw images.</p> <p>Parameters:</p> Name Type Description Default <code>raw_dir</code> <code>str | Path</code> <p>Path to raw data directory</p> <code>'data/raw'</code> <code>image_size</code> <code>int</code> <p>Target image size</p> <code>224</code> <p>Returns:</p> Type Description <code>tuple[Dataset, Dataset, Dataset]</code> <p>Tuple of (train_dataset, val_dataset, test_dataset)</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def chest_ct(\n    raw_dir: str | Path = \"data/raw\",\n    image_size: int = 224,\n) -&gt; tuple[Dataset, Dataset, Dataset]:\n    \"\"\"Convenience helper to load train, val, and test datasets from raw images.\n\n    Args:\n        raw_dir: Path to raw data directory\n        image_size: Target image size\n\n    Returns:\n        Tuple of (train_dataset, val_dataset, test_dataset)\n    \"\"\"\n    data_root = _find_data_root(Path(raw_dir))\n\n    train_ds = ChestCTDataset(data_root, split=\"train\", image_size=image_size)\n    val_ds = ChestCTDataset(data_root, split=\"valid\", image_size=image_size)\n    test_ds = ChestCTDataset(data_root, split=\"test\", image_size=image_size)\n\n    return train_ds, val_ds, test_ds\n</code></pre>"},{"location":"api-reference/data/#ct_scan_mlops.data.processed_chest_ct","title":"processed_chest_ct","text":"<pre><code>processed_chest_ct(\n    processed_dir: str | Path = \"data/processed\",\n) -&gt; tuple[Dataset, Dataset, Dataset]\n</code></pre> <p>Convenience helper to load train, val, and test datasets from processed tensors.</p> <p>Parameters:</p> Name Type Description Default <code>processed_dir</code> <code>str | Path</code> <p>Path to processed data directory</p> <code>'data/processed'</code> <p>Returns:</p> Type Description <code>tuple[Dataset, Dataset, Dataset]</code> <p>Tuple of (train_dataset, val_dataset, test_dataset)</p> Source code in <code>src/ct_scan_mlops/data.py</code> <pre><code>def processed_chest_ct(\n    processed_dir: str | Path = \"data/processed\",\n) -&gt; tuple[Dataset, Dataset, Dataset]:\n    \"\"\"Convenience helper to load train, val, and test datasets from processed tensors.\n\n    Args:\n        processed_dir: Path to processed data directory\n\n    Returns:\n        Tuple of (train_dataset, val_dataset, test_dataset)\n    \"\"\"\n    processed_path = Path(processed_dir)\n\n    train_ds = ProcessedChestCTDataset(processed_path, split=\"train\")\n    val_ds = ProcessedChestCTDataset(processed_path, split=\"valid\")\n    test_ds = ProcessedChestCTDataset(processed_path, split=\"test\")\n\n    return train_ds, val_ds, test_ds\n</code></pre>"},{"location":"api-reference/data/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/data/#load-preprocessed-data","title":"Load Preprocessed Data","text":"<pre><code>from ct_scan_mlops.data import ProcessedChestCTDataset\n\ntrain_ds = ProcessedChestCTDataset(\"data/processed\", split=\"train\")\nval_ds = ProcessedChestCTDataset(\"data/processed\", split=\"valid\")\ntest_ds = ProcessedChestCTDataset(\"data/processed\", split=\"test\")\n</code></pre>"},{"location":"api-reference/data/#use-with-lightning","title":"Use with Lightning","text":"<pre><code>from ct_scan_mlops.data import ChestCTDataModule\n\ndatamodule = ChestCTDataModule(cfg)\ndatamodule.setup(stage=\"fit\")\n\ntrainer.fit(model, datamodule=datamodule)\n</code></pre>"},{"location":"api-reference/data/#preprocess-raw-data","title":"Preprocess Raw Data","text":"<pre><code>python -m ct_scan_mlops.data preprocess\n</code></pre>"},{"location":"api-reference/evaluate/","title":"Evaluation Module","text":"<p>Model evaluation and metrics.</p>"},{"location":"api-reference/evaluate/#overview","title":"Overview","text":"<p>The evaluation module provides:</p> <ul> <li>evaluate_model - Core evaluation logic with metrics</li> <li>load_model_from_checkpoint - Load models from various checkpoint formats</li> <li>CLI interface - Command-line evaluation with Typer</li> <li>Hydra integration - Automatic checkpoint discovery</li> </ul>"},{"location":"api-reference/evaluate/#functions","title":"Functions","text":""},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_model","title":"evaluate_model","text":"<pre><code>evaluate_model(\n    model: Module,\n    test_loader: DataLoader,\n    device: device,\n    log_to_wandb: bool = False,\n    save_confusion_matrix: bool = True,\n    output_dir: Path | None = None,\n) -&gt; dict[str, float]\n</code></pre> <p>Core evaluation logic. Can be called standalone or from a sweep wrapper.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to evaluate</p> required <code>test_loader</code> <code>DataLoader</code> <p>DataLoader for test data</p> required <code>device</code> <code>device</code> <p>Torch device to evaluate on</p> required <code>log_to_wandb</code> <code>bool</code> <p>Whether to log metrics to wandb (assumes wandb is initialized)</p> <code>False</code> <code>save_confusion_matrix</code> <code>bool</code> <p>Whether to save confusion matrix plot</p> <code>True</code> <code>output_dir</code> <code>Path | None</code> <p>Directory to save outputs (confusion matrix, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with test metrics (accuracy, per-class metrics)</p> Source code in <code>src/ct_scan_mlops/evaluate.py</code> <pre><code>def evaluate_model(\n    model: torch.nn.Module,\n    test_loader: torch.utils.data.DataLoader,\n    device: torch.device,\n    log_to_wandb: bool = False,\n    save_confusion_matrix: bool = True,\n    output_dir: Path | None = None,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Core evaluation logic. Can be called standalone or from a sweep wrapper.\n\n    Args:\n        model: The model to evaluate\n        test_loader: DataLoader for test data\n        device: Torch device to evaluate on\n        log_to_wandb: Whether to log metrics to wandb (assumes wandb is initialized)\n        save_confusion_matrix: Whether to save confusion matrix plot\n        output_dir: Directory to save outputs (confusion matrix, etc.)\n\n    Returns:\n        Dictionary with test metrics (accuracy, per-class metrics)\n    \"\"\"\n    model.eval()\n    correct, total = 0, 0\n    all_preds = []\n    all_targets = []\n\n    logger.info(\"Running evaluation on test set...\")\n\n    with torch.no_grad():\n        for images, targets in test_loader:\n            images, targets = images.to(device), targets.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(dim=1)\n\n            correct += (preds == targets).sum().item()\n            total += targets.size(0)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    test_accuracy = correct / total\n    logger.info(f\"Test accuracy: {test_accuracy:.4f} ({correct}/{total})\")\n\n    # Compute detailed metrics\n    metrics = {\"test_accuracy\": test_accuracy}\n\n    # Classification report\n    report = classification_report(\n        all_targets,\n        all_preds,\n        target_names=CLASSES,\n        output_dict=True,\n        zero_division=0,\n    )\n\n    # Extract per-class metrics\n    for class_name in CLASSES:\n        if class_name in report:\n            metrics[f\"test_{class_name}_precision\"] = report[class_name][\"precision\"]\n            metrics[f\"test_{class_name}_recall\"] = report[class_name][\"recall\"]\n            metrics[f\"test_{class_name}_f1\"] = report[class_name][\"f1-score\"]\n\n    # Overall metrics\n    metrics[\"test_macro_avg_f1\"] = report[\"macro avg\"][\"f1-score\"]\n    metrics[\"test_weighted_avg_f1\"] = report[\"weighted avg\"][\"f1-score\"]\n\n    # Print classification report\n    logger.info(\"\\nClassification Report:\")\n    report_str = classification_report(\n        all_targets,\n        all_preds,\n        target_names=CLASSES,\n        zero_division=0,\n    )\n    print(report_str)\n\n    # Confusion matrix\n    cm = confusion_matrix(all_targets, all_preds)\n    logger.info(\"\\nConfusion Matrix:\")\n    logger.info(f\"\\n{cm}\")\n\n    # Save confusion matrix plot\n    if save_confusion_matrix and output_dir is not None:\n        output_dir.mkdir(parents=True, exist_ok=True)\n        fig, ax = plt.subplots(figsize=(10, 8))\n        sns.heatmap(\n            cm,\n            annot=True,\n            fmt=\"d\",\n            cmap=\"Blues\",\n            xticklabels=CLASSES,\n            yticklabels=CLASSES,\n            ax=ax,\n        )\n        ax.set_xlabel(\"Predicted\")\n        ax.set_ylabel(\"True\")\n        ax.set_title(f\"Confusion Matrix (Accuracy: {test_accuracy:.4f})\")\n        plt.tight_layout()\n\n        cm_path = output_dir / \"confusion_matrix.png\"\n        fig.savefig(cm_path, dpi=150)\n        plt.close(fig)\n        logger.info(f\"Confusion matrix saved to {cm_path}\")\n\n        if log_to_wandb:\n            wandb.log({\"confusion_matrix\": wandb.Image(str(cm_path))})\n\n    # Log to wandb if requested\n    if log_to_wandb:\n        wandb.log(metrics)\n        # Also set as summary metrics\n        for key, value in metrics.items():\n            wandb.run.summary[key] = value\n\n    return metrics\n</code></pre>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.load_model_from_checkpoint","title":"load_model_from_checkpoint","text":"<pre><code>load_model_from_checkpoint(\n    checkpoint_path: Path, cfg: DictConfig, device: device\n) -&gt; torch.nn.Module\n</code></pre> <p>Load model from checkpoint file.</p> <p>Handles multiple checkpoint formats: - Lightning .ckpt format (state_dict key) - Full checkpoint format (model_state_dict key) - Simple state_dict format (.pt file)</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Path</code> <p>Path to checkpoint file (.pt or .ckpt)</p> required <code>cfg</code> <code>DictConfig</code> <p>Hydra config for building the model</p> required <code>device</code> <code>device</code> <p>Device to load model on</p> required <p>Returns:</p> Type Description <code>Module</code> <p>Loaded model</p> Source code in <code>src/ct_scan_mlops/evaluate.py</code> <pre><code>def load_model_from_checkpoint(\n    checkpoint_path: Path,\n    cfg: DictConfig,\n    device: torch.device,\n) -&gt; torch.nn.Module:\n    \"\"\"Load model from checkpoint file.\n\n    Handles multiple checkpoint formats:\n    - Lightning .ckpt format (state_dict key)\n    - Full checkpoint format (model_state_dict key)\n    - Simple state_dict format (.pt file)\n\n    Args:\n        checkpoint_path: Path to checkpoint file (.pt or .ckpt)\n        cfg: Hydra config for building the model\n        device: Device to load model on\n\n    Returns:\n        Loaded model\n    \"\"\"\n    logger.info(f\"Loading model from {checkpoint_path}\")\n\n    # Build model from config\n    model = build_model(cfg).to(device)\n\n    # Load checkpoint with weights_only=True for security (supports Lightning checkpoints)\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n\n    # Handle different checkpoint formats\n    if isinstance(checkpoint, dict):\n        if \"state_dict\" in checkpoint:\n            # Lightning .ckpt format - model weights are under 'model.' prefix\n            state_dict = checkpoint[\"state_dict\"]\n            # Remove 'model.' prefix from Lightning checkpoint keys\n            model_state_dict = {}\n            for key, value in state_dict.items():\n                if key.startswith(\"model.\"):\n                    model_state_dict[key[6:]] = value  # Remove 'model.' prefix\n                else:\n                    model_state_dict[key] = value\n            model.load_state_dict(model_state_dict)\n            logger.info(f\"Loaded Lightning checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n        elif \"model_state_dict\" in checkpoint:\n            # Full checkpoint format (with optimizer, epoch, etc.)\n            model.load_state_dict(checkpoint[\"model_state_dict\"])\n            logger.info(f\"Loaded checkpoint from epoch {checkpoint.get('epoch', 'unknown')}\")\n            if \"val_acc\" in checkpoint:\n                logger.info(f\"Checkpoint validation accuracy: {checkpoint['val_acc']:.4f}\")\n        else:\n            # Dict but no recognized key - treat as raw state_dict\n            model.load_state_dict(checkpoint)\n    else:\n        # Simple state_dict format\n        model.load_state_dict(checkpoint)\n\n    return model\n</code></pre>"},{"location":"api-reference/evaluate/#cli-commands","title":"CLI Commands","text":""},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_cli","title":"evaluate_cli","text":"<pre><code>evaluate_cli(\n    checkpoint: str = typer.Argument(\n        ..., help=\"Path to model checkpoint (.ckpt or .pt)\"\n    ),\n    config_path: str = typer.Option(\n        None,\n        \"--config\",\n        \"-c\",\n        help=\"Path to training config.yaml (default: uses default config)\",\n    ),\n    use_wandb: bool = typer.Option(\n        False, \"--wandb\", help=\"Log results to W&amp;B\"\n    ),\n    wandb_project: str = typer.Option(\n        \"CT_Scan_MLOps\", help=\"W&amp;B project name\"\n    ),\n    wandb_entity: str = typer.Option(\n        None, help=\"W&amp;B entity (username or team)\"\n    ),\n    batch_size: int = typer.Option(\n        None,\n        \"--batch-size\",\n        \"-b\",\n        help=\"Batch size for evaluation (default: from config)\",\n    ),\n    output_dir: str = typer.Option(\n        None,\n        \"--output\",\n        \"-o\",\n        help=\"Output directory for plots (default: same as checkpoint)\",\n    ),\n) -&gt; dict[str, float]\n</code></pre> <p>Evaluate a trained model on the test set (standalone CLI entry point).</p> <p>Supports both Lightning checkpoints (.ckpt) and PyTorch state dicts (.pt).</p> <p>Examples:</p>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_cli--basic-evaluation-lightning-checkpoint","title":"Basic evaluation (Lightning checkpoint)","text":"<p>invoke evaluate --checkpoint outputs/ct_scan_classifier/2024-01-14/12-34-56/best_model.ckpt</p>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_cli--evaluate-pytorch-state-dict","title":"Evaluate PyTorch state dict","text":"<p>invoke evaluate --checkpoint outputs/.../model.pt</p>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_cli--with-wandb-logging","title":"With wandb logging","text":"<p>invoke evaluate --checkpoint outputs/.../best_model.ckpt --wandb --wandb-entity YOUR_USERNAME</p>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_cli--custom-batch-size","title":"Custom batch size","text":"<p>invoke evaluate --checkpoint models/best_model.ckpt --batch-size 64</p> Source code in <code>src/ct_scan_mlops/evaluate.py</code> <pre><code>@app.command()\ndef evaluate_cli(\n    checkpoint: str = typer.Argument(..., help=\"Path to model checkpoint (.ckpt or .pt)\"),\n    config_path: str = typer.Option(\n        None,\n        \"--config\",\n        \"-c\",\n        help=\"Path to training config.yaml (default: uses default config)\",\n    ),\n    use_wandb: bool = typer.Option(False, \"--wandb\", help=\"Log results to W&amp;B\"),\n    wandb_project: str = typer.Option(\"CT_Scan_MLOps\", help=\"W&amp;B project name\"),\n    wandb_entity: str = typer.Option(None, help=\"W&amp;B entity (username or team)\"),\n    batch_size: int = typer.Option(None, \"--batch-size\", \"-b\", help=\"Batch size for evaluation (default: from config)\"),\n    output_dir: str = typer.Option(\n        None, \"--output\", \"-o\", help=\"Output directory for plots (default: same as checkpoint)\"\n    ),\n) -&gt; dict[str, float]:\n    \"\"\"Evaluate a trained model on the test set (standalone CLI entry point).\n\n    Supports both Lightning checkpoints (.ckpt) and PyTorch state dicts (.pt).\n\n    Examples:\n        # Basic evaluation (Lightning checkpoint)\n        invoke evaluate --checkpoint outputs/ct_scan_classifier/2024-01-14/12-34-56/best_model.ckpt\n\n        # Evaluate PyTorch state dict\n        invoke evaluate --checkpoint outputs/.../model.pt\n\n        # With wandb logging\n        invoke evaluate --checkpoint outputs/.../best_model.ckpt --wandb --wandb-entity YOUR_USERNAME\n\n        # Custom batch size\n        invoke evaluate --checkpoint models/best_model.ckpt --batch-size 64\n    \"\"\"\n    checkpoint_path = Path(checkpoint)\n    if not checkpoint_path.exists():\n        logger.error(f\"Checkpoint not found: {checkpoint_path}\")\n        raise typer.Exit(1)\n\n    # Load config\n    if config_path:\n        cfg = OmegaConf.load(config_path)\n    else:\n        # Load default config\n        logger.info(\"Using default config from configs/\")\n        with hydra.initialize(config_path=_CONFIG_PATH, version_base=\"1.3\"):\n            cfg = hydra.compose(config_name=\"config\")\n\n    # Override batch size if provided\n    if batch_size:\n        cfg.data.batch_size = batch_size\n\n    # Set output directory\n    out_dir = Path(output_dir) if output_dir else checkpoint_path.parent\n\n    logger.info(f\"Output directory: {out_dir}\")\n\n    # Initialize W&amp;B if requested\n    if use_wandb:\n        wandb.init(\n            project=wandb_project,\n            entity=wandb_entity,\n            job_type=\"eval\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            name=f\"eval_{cfg.model.name}_{checkpoint_path.stem}\",\n        )\n\n    # Get device\n    device = get_device()\n    logger.info(f\"Using device: {device}\")\n\n    # Load model\n    model = load_model_from_checkpoint(checkpoint_path, cfg, device)\n\n    # Create dataloaders\n    _, _, test_loader = create_dataloaders(cfg, use_processed=True)\n\n    # Evaluate\n    metrics = evaluate_model(\n        model=model,\n        test_loader=test_loader,\n        device=device,\n        log_to_wandb=use_wandb,\n        save_confusion_matrix=True,\n        output_dir=out_dir,\n    )\n\n    if use_wandb:\n        wandb.finish()\n\n    return metrics\n</code></pre>"},{"location":"api-reference/evaluate/#ct_scan_mlops.evaluate.evaluate_hydra","title":"evaluate_hydra","text":"<pre><code>evaluate_hydra(cfg: DictConfig) -&gt; dict[str, float] | None\n</code></pre> <p>Evaluate with Hydra config (for integration with training pipeline).</p> <p>This can be called after training completes or as a separate step. Automatically finds the best checkpoint from recent training runs.</p> Source code in <code>src/ct_scan_mlops/evaluate.py</code> <pre><code>@hydra.main(config_path=_CONFIG_PATH, config_name=\"config\", version_base=\"1.3\")\ndef evaluate_hydra(cfg: DictConfig) -&gt; dict[str, float] | None:\n    \"\"\"Evaluate with Hydra config (for integration with training pipeline).\n\n    This can be called after training completes or as a separate step.\n    Automatically finds the best checkpoint from recent training runs.\n    \"\"\"\n    device = get_device()\n    logger.info(f\"Using device: {device}\")\n\n    # Look for checkpoints in the most recent output directory\n    output_base = Path(\"outputs\") / cfg.experiment_name\n    checkpoint_path = None\n\n    if output_base.exists():\n        # Find most recent run\n        run_dirs = sorted(output_base.glob(\"*/*\"), key=lambda p: p.stat().st_mtime, reverse=True)\n        if run_dirs:\n            run_dir = run_dirs[0]\n            # Try Lightning checkpoint first (.ckpt), then PyTorch (.pt)\n            for filename in [\"best_model.ckpt\", \"best_model.pt\", \"model.pt\", \"last.ckpt\"]:\n                candidate = run_dir / filename\n                if candidate.exists():\n                    checkpoint_path = candidate\n                    logger.info(f\"Found checkpoint: {checkpoint_path}\")\n                    break\n\n            if checkpoint_path is None:\n                logger.error(f\"No checkpoint found in {run_dir}\")\n                return None\n        else:\n            logger.error(f\"No training runs found in {output_base}\")\n            return None\n    else:\n        logger.error(f\"Output directory not found: {output_base}\")\n        return None\n\n    # Load model\n    model = load_model_from_checkpoint(checkpoint_path, cfg, device)\n\n    # Create dataloaders\n    _, _, test_loader = create_dataloaders(cfg, use_processed=True)\n\n    # Evaluate\n    return evaluate_model(\n        model=model,\n        test_loader=test_loader,\n        device=device,\n        log_to_wandb=False,\n        save_confusion_matrix=True,\n        output_dir=checkpoint_path.parent,\n    )\n</code></pre>"},{"location":"api-reference/evaluate/#metrics","title":"Metrics","text":"<p>The evaluator computes:</p> Metric Description <code>test_accuracy</code> Overall classification accuracy <code>test_{class}_precision</code> Per-class precision <code>test_{class}_recall</code> Per-class recall <code>test_{class}_f1</code> Per-class F1 score <code>test_macro_avg_f1</code> Macro-averaged F1 <code>test_weighted_avg_f1</code> Weighted-averaged F1"},{"location":"api-reference/evaluate/#checkpoint-formats","title":"Checkpoint Formats","text":"<p>The loader supports multiple formats:</p> Format Key Description Lightning <code>state_dict</code> Full Lightning checkpoint with <code>model.</code> prefix Full <code>model_state_dict</code> Custom checkpoint with optimizer state Simple (raw dict) Plain PyTorch state dict"},{"location":"api-reference/evaluate/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/evaluate/#cli-evaluation","title":"CLI Evaluation","text":"<pre><code># Basic\ninvoke evaluate --checkpoint outputs/.../best_model.ckpt\n\n# With W&amp;B logging\ninvoke evaluate --checkpoint path/to/model.ckpt --wandb --wandb-entity YOUR_USERNAME\n\n# Custom batch size\ninvoke evaluate --checkpoint path/to/model.ckpt --batch-size 64\n</code></pre>"},{"location":"api-reference/evaluate/#programmatic-evaluation","title":"Programmatic Evaluation","text":"<pre><code>from pathlib import Path\nfrom ct_scan_mlops.evaluate import evaluate_model, load_model_from_checkpoint\nfrom ct_scan_mlops.data import create_dataloaders\nfrom ct_scan_mlops.utils import get_device\n\n# Load model\ndevice = get_device()\nmodel = load_model_from_checkpoint(Path(\"model.ckpt\"), cfg, device)\n\n# Create test dataloader\n_, _, test_loader = create_dataloaders(cfg)\n\n# Evaluate\nmetrics = evaluate_model(\n    model=model,\n    test_loader=test_loader,\n    device=device,\n    save_confusion_matrix=True,\n    output_dir=Path(\"results\"),\n)\n\nprint(f\"Test accuracy: {metrics['test_accuracy']:.4f}\")\n</code></pre>"},{"location":"api-reference/evaluate/#hydra-based-evaluation","title":"Hydra-based Evaluation","text":"<pre><code>uv run python -m ct_scan_mlops.evaluate\n</code></pre> <p>This automatically finds the most recent checkpoint.</p>"},{"location":"api-reference/model/","title":"Model Module","text":"<p>Neural network architectures for CT scan classification.</p>"},{"location":"api-reference/model/#overview","title":"Overview","text":"<p>The model module provides:</p> <ul> <li>CustomCNN - A configurable CNN baseline</li> <li>ResNet18 - Transfer learning with pretrained weights</li> <li>Model registry - Extensible system for adding new models</li> </ul>"},{"location":"api-reference/model/#registry","title":"Registry","text":"<p>Models are registered using the <code>@register_model</code> decorator and can be instantiated via <code>build_model()</code>.</p>"},{"location":"api-reference/model/#ct_scan_mlops.model.build_model","title":"build_model","text":"<pre><code>build_model(cfg: DictConfig) -&gt; nn.Module\n</code></pre> <p>Build model from Hydra config using the model registry.</p> <p>Models are registered with the @register_model decorator and must implement a from_config(cfg) class method for instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra config containing model parameters.  Expected to have cfg.model.name specifying which model to build.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>Configured model instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model name is not found in the registry.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def build_model(cfg: DictConfig) -&gt; nn.Module:\n    \"\"\"Build model from Hydra config using the model registry.\n\n    Models are registered with the @register_model decorator and must implement\n    a from_config(cfg) class method for instantiation.\n\n    Args:\n        cfg: Hydra config containing model parameters.\n             Expected to have cfg.model.name specifying which model to build.\n\n    Returns:\n        Configured model instance.\n\n    Raises:\n        ValueError: If the model name is not found in the registry.\n    \"\"\"\n    name = cfg.model.name.lower()\n\n    if name not in MODEL_REGISTRY:\n        available = \", \".join(sorted(MODEL_REGISTRY.keys()))\n        raise ValueError(f\"Unknown model: '{name}'. Available models: {available}\")\n\n    model_cls = MODEL_REGISTRY[name]\n    return model_cls.from_config(cfg)\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.register_model","title":"register_model","text":"<pre><code>register_model(\n    *names: str,\n) -&gt; Callable[[type[nn.Module]], type[nn.Module]]\n</code></pre> <p>Decorator to register a model class with one or more names.</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <code>str</code> <p>One or more names to register the model under (case-insensitive).</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[type[Module]], type[Module]]</code> <p>Decorator function that registers the class and returns it unchanged.</p> Example <p>@register_model(\"custom_cnn\", \"cnn\") class CustomCNN(nn.Module):     ...</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def register_model(*names: str) -&gt; Callable[[type[nn.Module]], type[nn.Module]]:\n    \"\"\"Decorator to register a model class with one or more names.\n\n    Args:\n        *names: One or more names to register the model under (case-insensitive).\n\n    Returns:\n        Decorator function that registers the class and returns it unchanged.\n\n    Example:\n        @register_model(\"custom_cnn\", \"cnn\")\n        class CustomCNN(nn.Module):\n            ...\n    \"\"\"\n\n    def decorator(cls: type[nn.Module]) -&gt; type[nn.Module]:\n        for name in names:\n            MODEL_REGISTRY[name.lower()] = cls\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api-reference/model/#models","title":"Models","text":""},{"location":"api-reference/model/#ct_scan_mlops.model.CustomCNN","title":"CustomCNN","text":"<pre><code>CustomCNN(\n    num_classes: int = 4,\n    in_channels: int = 3,\n    hidden_dims: list[int] | None = None,\n    fc_hidden: int = 512,\n    dropout: float = 0.3,\n    batch_norm: bool = True,\n    kernel_size: int = 3,\n    image_size: int = 224,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Configurable CNN for image classification.</p> <p>Architecture is fully configurable via constructor parameters, designed to work with Hydra configs.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int = 4,\n    in_channels: int = 3,\n    hidden_dims: list[int] | None = None,\n    fc_hidden: int = 512,\n    dropout: float = 0.3,\n    batch_norm: bool = True,\n    kernel_size: int = 3,\n    image_size: int = 224,\n) -&gt; None:\n    super().__init__()\n\n    if hidden_dims is None:\n        hidden_dims = [32, 64, 128, 256]\n\n    self.hidden_dims = hidden_dims\n    self.batch_norm = batch_norm\n    padding = kernel_size // 2\n\n    # Build convolutional layers dynamically\n    layers: list[nn.Module] = []\n    prev_channels = in_channels\n\n    for out_channels in hidden_dims:\n        layers.append(nn.Conv2d(prev_channels, out_channels, kernel_size=kernel_size, padding=padding))\n        if batch_norm:\n            layers.append(nn.BatchNorm2d(out_channels))\n        layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.MaxPool2d(2))  # Halves spatial dimensions\n        prev_channels = out_channels\n\n    self.features = nn.Sequential(*layers)\n\n    # Calculate flattened size after conv layers\n    # Each MaxPool2d halves the spatial dimensions\n    num_pools = len(hidden_dims)\n    final_spatial = image_size // (2**num_pools)\n    flatten_size = hidden_dims[-1] * final_spatial * final_spatial\n\n    self.classifier = nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(flatten_size, fc_hidden),\n        nn.ReLU(inplace=True),\n        nn.Dropout(dropout),\n        nn.Linear(fc_hidden, num_classes),\n    )\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.CustomCNN.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.classifier(self.features(x))\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.CustomCNN.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(cfg: DictConfig) -&gt; CustomCNN\n</code></pre> <p>Create CustomCNN from Hydra config.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>@classmethod\ndef from_config(cls, cfg: DictConfig) -&gt; CustomCNN:\n    \"\"\"Create CustomCNN from Hydra config.\"\"\"\n    model_cfg = cfg.model\n    return cls(\n        num_classes=model_cfg.num_classes,\n        in_channels=model_cfg.get(\"input_channels\", 3),\n        hidden_dims=list(model_cfg.hidden_dims),\n        fc_hidden=model_cfg.fc_hidden,\n        dropout=model_cfg.dropout,\n        batch_norm=model_cfg.batch_norm,\n        image_size=cfg.data.image_size,\n    )\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.ResNet18","title":"ResNet18","text":"<pre><code>ResNet18(\n    num_classes: int = 4,\n    pretrained: bool = True,\n    freeze_backbone: bool = False,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>ResNet18 for transfer learning.</p> <p>Supports freezing/unfreezing backbone for fine-tuning strategies.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def __init__(\n    self,\n    num_classes: int = 4,\n    pretrained: bool = True,\n    freeze_backbone: bool = False,\n) -&gt; None:\n    super().__init__()\n\n    # Load pretrained model\n    if pretrained:\n        weights = models.ResNet18_Weights.DEFAULT\n        self.backbone = models.resnet18(weights=weights)\n    else:\n        self.backbone = models.resnet18(weights=None)\n\n    # Replace classification head\n    in_features = self.backbone.fc.in_features\n    self.backbone.fc = nn.Linear(in_features, num_classes)\n\n    # Optionally freeze backbone\n    if freeze_backbone:\n        self.freeze_backbone()\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.ResNet18.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.backbone(x)\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.ResNet18.from_config","title":"from_config  <code>classmethod</code>","text":"<pre><code>from_config(cfg: DictConfig) -&gt; ResNet18\n</code></pre> <p>Create ResNet18 from Hydra config.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>@classmethod\ndef from_config(cls, cfg: DictConfig) -&gt; ResNet18:\n    \"\"\"Create ResNet18 from Hydra config.\"\"\"\n    model_cfg = cfg.model\n    return cls(\n        num_classes=model_cfg.num_classes,\n        pretrained=model_cfg.get(\"pretrained\", True),\n        freeze_backbone=model_cfg.get(\"freeze_backbone\", False),\n    )\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.ResNet18.freeze_backbone","title":"freeze_backbone","text":"<pre><code>freeze_backbone() -&gt; None\n</code></pre> <p>Freeze all layers except the classification head.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def freeze_backbone(self) -&gt; None:\n    \"\"\"Freeze all layers except the classification head.\"\"\"\n    for name, param in self.backbone.named_parameters():\n        if \"fc\" not in name:\n            param.requires_grad = False\n</code></pre>"},{"location":"api-reference/model/#ct_scan_mlops.model.ResNet18.unfreeze_backbone","title":"unfreeze_backbone","text":"<pre><code>unfreeze_backbone() -&gt; None\n</code></pre> <p>Unfreeze all layers.</p> Source code in <code>src/ct_scan_mlops/model.py</code> <pre><code>def unfreeze_backbone(self) -&gt; None:\n    \"\"\"Unfreeze all layers.\"\"\"\n    for param in self.backbone.parameters():\n        param.requires_grad = True\n</code></pre>"},{"location":"api-reference/model/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/model/#build-from-config","title":"Build from Config","text":"<pre><code>from ct_scan_mlops.model import build_model\n\nmodel = build_model(cfg)  # cfg.model.name determines which model\n</code></pre>"},{"location":"api-reference/model/#direct-instantiation","title":"Direct Instantiation","text":"<pre><code>from ct_scan_mlops.model import CustomCNN, ResNet18\n\n# CustomCNN\ncnn = CustomCNN(\n    num_classes=4,\n    hidden_dims=[32, 64, 128, 256],\n    fc_hidden=512,\n    dropout=0.3,\n)\n\n# ResNet18 with transfer learning\nresnet = ResNet18(\n    num_classes=4,\n    pretrained=True,\n    freeze_backbone=False,\n)\n</code></pre>"},{"location":"api-reference/model/#fine-tuning-resnet18","title":"Fine-tuning ResNet18","text":"<pre><code>from ct_scan_mlops.model import ResNet18\n\n# Start with frozen backbone\nmodel = ResNet18(num_classes=4, freeze_backbone=True)\n\n# Train classification head only\ntrain(model, epochs=10)\n\n# Unfreeze and fine-tune entire model\nmodel.unfreeze_backbone()\ntrain(model, epochs=20, lr=1e-5)\n</code></pre>"},{"location":"api-reference/model/#adding-new-models","title":"Adding New Models","text":"<ol> <li>Define your model class with <code>from_config</code> classmethod</li> <li>Register it with the <code>@register_model</code> decorator</li> </ol> <pre><code>from ct_scan_mlops.model import register_model\n\n@register_model(\"my_model\")\nclass MyModel(nn.Module):\n    def __init__(self, num_classes: int = 4):\n        super().__init__()\n        # ... define layers ...\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # ... forward pass ...\n\n    @classmethod\n    def from_config(cls, cfg: DictConfig) -&gt; \"MyModel\":\n        return cls(num_classes=cfg.model.num_classes)\n</code></pre> <p>Then create <code>configs/model/my_model.yaml</code>:</p> <pre><code>name: my_model\nnum_classes: 4\n# ... other params ...\n</code></pre> <p>Use it:</p> <pre><code>invoke train --args \"model=my_model\"\n</code></pre>"},{"location":"api-reference/train/","title":"Training Module","text":"<p>Training pipeline with PyTorch Lightning.</p>"},{"location":"api-reference/train/#overview","title":"Overview","text":"<p>The training module provides:</p> <ul> <li>LitModel - Lightning module wrapping the classifier</li> <li>train_model - Main training function with full MLOps features</li> <li>Logging - Configurable file and console logging</li> <li>Reproducibility - Seed management for reproducible training</li> </ul>"},{"location":"api-reference/train/#lightning-module","title":"Lightning Module","text":""},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel","title":"LitModel","text":"<pre><code>LitModel(cfg: DictConfig)\n</code></pre> <p>               Bases: <code>LightningModule</code></p> <p>Lightning module wrapping the CT scan classifier.</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def __init__(self, cfg: DictConfig):\n    super().__init__()\n    self.cfg = cfg\n    self.model = build_model(cfg)\n    self.criterion = torch.nn.CrossEntropyLoss()\n    self.save_hyperparameters(ignore=[\"model\"])\n\n    # Track training history for plotting\n    self.training_history: dict[str, list[float]] = {\n        \"train_loss\": [],\n        \"train_acc\": [],\n        \"val_loss\": [],\n        \"val_acc\": [],\n        \"lr\": [],\n    }\n\n    # Cache profiling config (parsed once, not every training step)\n    profiling_cfg = cfg.get(\"train\", {}).get(\"profiling\", {})\n    self._profiling_enabled = bool(profiling_cfg.get(\"enabled\", True))\n    self._profiling_steps = int(profiling_cfg.get(\"steps\", 5))\n    self._profiled = False\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; torch.Tensor\n</code></pre> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    return self.model(x)\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.training_step","title":"training_step","text":"<pre><code>training_step(batch, batch_idx: int)\n</code></pre> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def training_step(self, batch, batch_idx: int):\n    x, y = batch\n\n    # Run profiling once on first batch of first epoch (if enabled)\n    if self._profiling_enabled and not self._profiled and batch_idx == 0 and self.current_epoch == 0:\n        with profile(\n            activities=[ProfilerActivity.CPU],\n            record_shapes=True,\n            with_stack=True,\n            on_trace_ready=tensorboard_trace_handler(str(profiling_dir)),\n        ) as prof:\n            for _ in range(max(1, self._profiling_steps)):\n                y_hat = self(x)\n                loss = self.criterion(y_hat, y)\n                prof.step()\n\n        print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))\n        self._profiled = True\n    else:\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n\n    acc = self._compute_accuracy(y_hat, y)\n\n    self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n    self.log(\"train_acc\", acc, on_epoch=True, prog_bar=True)\n\n    return loss\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.validation_step","title":"validation_step","text":"<pre><code>validation_step(batch, batch_idx: int)\n</code></pre> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def validation_step(self, batch, batch_idx: int):\n    x, y = batch\n    y_hat = self(x)\n    loss = self.criterion(y_hat, y)\n    acc = self._compute_accuracy(y_hat, y)\n\n    self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n    self.log(\"val_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n\n    return loss\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.test_step","title":"test_step","text":"<pre><code>test_step(batch, batch_idx: int)\n</code></pre> <p>Test step for model evaluation.</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def test_step(self, batch, batch_idx: int):\n    \"\"\"Test step for model evaluation.\"\"\"\n    x, y = batch\n    y_hat = self(x)\n    loss = self.criterion(y_hat, y)\n    acc = self._compute_accuracy(y_hat, y)\n\n    self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n    self.log(\"test_acc\", acc, on_step=False, on_epoch=True)\n\n    return loss\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.configure_optimizers","title":"configure_optimizers","text":"<pre><code>configure_optimizers()\n</code></pre> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def configure_optimizers(self):\n    opt_cfg = self.cfg.train.optimizer\n    sched_cfg = self.cfg.train.scheduler\n\n    optimizer = torch.optim.Adam(\n        self.parameters(),\n        lr=opt_cfg.lr,\n        weight_decay=opt_cfg.weight_decay,\n        betas=tuple(opt_cfg.betas),\n    )\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=self.cfg.train.max_epochs,\n        eta_min=sched_cfg.eta_min,\n    )\n\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\n            \"scheduler\": scheduler,\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        },\n    }\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.on_train_start","title":"on_train_start","text":"<pre><code>on_train_start()\n</code></pre> <p>Log sample images at training start.</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def on_train_start(self):\n    \"\"\"Log sample images at training start.\"\"\"\n    if self.trainer.train_dataloader is None or self.logger is None:\n        return\n\n    # Only log images if using WandbLogger\n    if not isinstance(self.logger, WandbLogger):\n        return\n\n    batch = next(iter(self.trainer.train_dataloader))\n    x, y = batch\n    self.logger.experiment.log({\"examples\": [wandb.Image(x[j].cpu()) for j in range(min(8, len(x)))]})\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.LitModel.on_train_epoch_end","title":"on_train_epoch_end","text":"<pre><code>on_train_epoch_end()\n</code></pre> <p>Track metrics at end of each epoch for plotting.</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def on_train_epoch_end(self):\n    \"\"\"Track metrics at end of each epoch for plotting.\"\"\"\n    metrics = self.trainer.callback_metrics\n    self.training_history[\"train_loss\"].append(\n        metrics.get(\"train_loss\", 0).item()\n        if torch.is_tensor(metrics.get(\"train_loss\", 0))\n        else metrics.get(\"train_loss\", 0)\n    )\n    self.training_history[\"train_acc\"].append(\n        metrics.get(\"train_acc\", 0).item()\n        if torch.is_tensor(metrics.get(\"train_acc\", 0))\n        else metrics.get(\"train_acc\", 0)\n    )\n    self.training_history[\"val_loss\"].append(\n        metrics.get(\"val_loss\", 0).item()\n        if torch.is_tensor(metrics.get(\"val_loss\", 0))\n        else metrics.get(\"val_loss\", 0)\n    )\n    self.training_history[\"val_acc\"].append(\n        metrics.get(\"val_acc\", 0).item()\n        if torch.is_tensor(metrics.get(\"val_acc\", 0))\n        else metrics.get(\"val_acc\", 0)\n    )\n\n    # Get current LR from optimizer\n    if self.trainer.optimizers:\n        current_lr = self.trainer.optimizers[0].param_groups[0][\"lr\"]\n        self.training_history[\"lr\"].append(current_lr)\n</code></pre>"},{"location":"api-reference/train/#training-functions","title":"Training Functions","text":""},{"location":"api-reference/train/#ct_scan_mlops.train.train_model","title":"train_model","text":"<pre><code>train_model(\n    cfg: DictConfig,\n    output_dir: str,\n    wandb_logger: WandbLogger | None = None,\n) -&gt; str\n</code></pre> <p>Train using PyTorch Lightning with full MLOps features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration</p> required <code>output_dir</code> <code>str</code> <p>Directory to save outputs</p> required <code>wandb_logger</code> <code>WandbLogger | None</code> <p>WandbLogger instance for experiment tracking</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to saved model checkpoint</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def train_model(\n    cfg: DictConfig,\n    output_dir: str,\n    wandb_logger: WandbLogger | None = None,\n) -&gt; str:\n    \"\"\"Train using PyTorch Lightning with full MLOps features.\n\n    Args:\n        cfg: Hydra configuration\n        output_dir: Directory to save outputs\n        wandb_logger: WandbLogger instance for experiment tracking\n\n    Returns:\n        Path to saved model checkpoint\n    \"\"\"\n    output_path = Path(output_dir)\n    train_cfg = cfg.train\n\n    logger.info(f\"Configuration:\\n{OmegaConf.to_yaml(cfg)}\")\n\n    # Seed for reproducibility\n    set_seed(cfg.seed, get_device())\n    pl.seed_everything(cfg.seed, workers=True)\n\n    # Data - using LightningDataModule for best practices\n    # Note: Trainer.fit() calls datamodule.setup() automatically, but we call it here\n    # to log dataset sizes before training starts\n    datamodule = ChestCTDataModule(cfg)\n    datamodule.setup(stage=\"fit\")\n    logger.info(f\"Train batches: {len(datamodule.train_dataloader())}, Val batches: {len(datamodule.val_dataloader())}\")\n\n    # Lightning model\n    lit_model = LitModel(cfg)\n\n    # Log model info\n    total_params = sum(p.numel() for p in lit_model.model.parameters())\n    trainable_params = sum(p.numel() for p in lit_model.model.parameters() if p.requires_grad)\n    logger.info(f\"Model: {cfg.model.name} | Total params: {total_params:,} | Trainable: {trainable_params:,}\")\n\n    # ---- Callbacks ----\n    callbacks = []\n\n    # ModelCheckpoint - saves best model based on validation metric\n    ckpt_cfg = train_cfg.checkpoint\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=str(output_path),\n        filename=\"best_model\",\n        monitor=ckpt_cfg.monitor,\n        mode=ckpt_cfg.mode,\n        save_top_k=ckpt_cfg.save_top_k,\n        save_last=ckpt_cfg.save_last,\n        verbose=True,\n    )\n    callbacks.append(checkpoint_callback)\n\n    # EarlyStopping - stops training when metric plateaus\n    es_cfg = train_cfg.early_stopping\n    if es_cfg.enabled:\n        early_stop_callback = EarlyStopping(\n            monitor=es_cfg.monitor,\n            patience=es_cfg.patience,\n            mode=es_cfg.mode,\n            min_delta=0.001,\n            verbose=True,\n        )\n        callbacks.append(early_stop_callback)\n        logger.info(f\"Early stopping enabled: monitor={es_cfg.monitor}, patience={es_cfg.patience}\")\n\n    # LearningRateMonitor - logs LR to W&amp;B\n    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n    callbacks.append(lr_monitor)\n\n    # ---- Trainer ----\n    trainer = pl.Trainer(\n        default_root_dir=str(output_path),\n        max_epochs=train_cfg.max_epochs,\n        min_epochs=train_cfg.get(\"min_epochs\", 1),\n        accelerator=train_cfg.get(\"accelerator\", \"auto\"),\n        devices=train_cfg.get(\"devices\", \"auto\"),\n        precision=train_cfg.get(\"precision\", 32),\n        gradient_clip_val=train_cfg.gradient_clip_val,\n        accumulate_grad_batches=train_cfg.get(\"accumulate_grad_batches\", 1),\n        log_every_n_steps=10,\n        callbacks=callbacks,\n        logger=wandb_logger,\n        enable_checkpointing=True,\n    )\n\n    # ---- Train ----\n    trainer.fit(lit_model, datamodule=datamodule)\n\n    # ---- Save final model weights (pure PyTorch format for easy loading) ----\n    final_model_path = output_path / \"model.pt\"\n    torch.save(lit_model.model.state_dict(), final_model_path)\n    logger.info(f\"Final model saved to {final_model_path}\")\n\n    # ---- Generate training curves ----\n    if lit_model.training_history[\"train_loss\"]:\n        _save_training_curves(lit_model.training_history, output_path, cfg, wandb_logger)\n\n    # ---- Log model artifact with rich metadata ----\n    best_val_acc = checkpoint_callback.best_model_score\n    best_val_acc_value = best_val_acc.item() if best_val_acc is not None else None\n\n    artifact = wandb.Artifact(\n        name=f\"{cfg.experiment_name}_model\",\n        type=\"model\",\n        description=f\"CT scan classifier trained for {trainer.current_epoch} epochs\",\n        metadata={\n            \"epochs_trained\": trainer.current_epoch,\n            \"best_val_acc\": best_val_acc_value,\n            \"final_train_loss\": lit_model.training_history[\"train_loss\"][-1]\n            if lit_model.training_history[\"train_loss\"]\n            else None,\n            \"final_val_loss\": lit_model.training_history[\"val_loss\"][-1]\n            if lit_model.training_history[\"val_loss\"]\n            else None,\n            \"model_name\": cfg.model.name,\n            \"num_params\": total_params,\n            \"trainable_params\": trainable_params,\n            \"seed\": cfg.seed,\n            **OmegaConf.to_container(cfg.model),\n        },\n    )\n\n    # Add best model checkpoint if it exists\n    best_ckpt_path = output_path / \"best_model.ckpt\"\n    if best_ckpt_path.exists():\n        artifact.add_file(str(best_ckpt_path))\n    artifact.add_file(str(final_model_path))\n\n    # Add the composed Hydra config if available\n    hydra_cfg_path = output_path / \".hydra\" / \"config.yaml\"\n    if hydra_cfg_path.exists():\n        artifact.add_file(str(hydra_cfg_path))\n\n    wandb.log_artifact(artifact)\n    logger.info(f\"Model artifact logged to W&amp;B: {artifact.name}\")\n\n    return str(final_model_path)\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.train","title":"train","text":"<pre><code>train(cfg: DictConfig) -&gt; None\n</code></pre> <p>Train a model (Hydra entry point).</p> Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>@hydra.main(config_path=_CONFIG_PATH, config_name=\"config\", version_base=\"1.3\")\ndef train(cfg: DictConfig) -&gt; None:\n    \"\"\"Train a model (Hydra entry point).\"\"\"\n    # Get Hydra's output directory\n    output_dir = hydra.core.hydra_config.HydraConfig.get().runtime.output_dir\n    configure_logging(output_dir)\n\n    device = get_device()\n    logger.info(f\"Training on {device}\")\n\n    # Initialize W&amp;B Logger for Lightning integration\n    wandb_cfg = cfg.wandb\n    wandb_logger = WandbLogger(\n        project=wandb_cfg.project,\n        entity=wandb_cfg.get(\"entity\"),\n        name=f\"{cfg.experiment_name}_{cfg.model.name}\",\n        config=OmegaConf.to_container(cfg, resolve=True),\n        tags=list(wandb_cfg.get(\"tags\", [])),\n        mode=wandb_cfg.get(\"mode\", \"online\"),\n        save_dir=output_dir,\n        job_type=\"train\",\n    )\n\n    logger.info(f\"W&amp;B run: {wandb_logger.experiment.url}\")\n\n    try:\n        model_path = train_model(cfg, output_dir, wandb_logger)\n        logger.info(f\"Training complete. Model saved to {model_path}\")\n\n        # Log final summary to W&amp;B\n        wandb_logger.experiment.summary[\"output_dir\"] = output_dir\n        wandb_logger.experiment.summary[\"model_path\"] = model_path\n\n    except Exception as e:\n        logger.exception(f\"Training failed: {e}\")\n        raise\n    finally:\n        wandb.finish()\n</code></pre>"},{"location":"api-reference/train/#utility-functions","title":"Utility Functions","text":""},{"location":"api-reference/train/#ct_scan_mlops.train.configure_logging","title":"configure_logging","text":"<pre><code>configure_logging(output_dir: str) -&gt; None\n</code></pre> <p>Configure loguru for file and console logging.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory to save log files</p> required Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def configure_logging(output_dir: str) -&gt; None:\n    \"\"\"Configure loguru for file and console logging.\n\n    Args:\n        output_dir: Directory to save log files\n    \"\"\"\n    logger.remove()  # Remove default handler\n\n    # File handler with rotation\n    log_path = Path(output_dir) / \"training.log\"\n    logger.add(\n        log_path,\n        level=\"DEBUG\",\n        rotation=\"100 MB\",\n        retention=5,\n        compression=\"gz\",\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: &lt;8} | {name}:{function}:{line} | {message}\",\n    )\n\n    # Console handler for INFO and above\n    logger.add(\n        sys.stderr,\n        level=\"INFO\",\n        format=\"&lt;green&gt;{time:HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | {message}\",\n    )\n\n    logger.info(f\"Logging configured. Logs saved to {log_path}\")\n</code></pre>"},{"location":"api-reference/train/#ct_scan_mlops.train.set_seed","title":"set_seed","text":"<pre><code>set_seed(seed: int, device: device) -&gt; None\n</code></pre> <p>Set random seeds for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value</p> required <code>device</code> <code>device</code> <p>Torch device being used</p> required Source code in <code>src/ct_scan_mlops/train.py</code> <pre><code>def set_seed(seed: int, device: torch.device) -&gt; None:\n    \"\"\"Set random seeds for reproducibility.\n\n    Args:\n        seed: Random seed value\n        device: Torch device being used\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)  # noqa: NPY002 - needed for legacy library compatibility\n    torch.manual_seed(seed)\n\n    if device.type == \"cuda\":\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    logger.info(f\"Random seed set to {seed}\")\n</code></pre>"},{"location":"api-reference/train/#training-features","title":"Training Features","text":""},{"location":"api-reference/train/#callbacks","title":"Callbacks","text":"<p>The training pipeline includes:</p> <ul> <li>ModelCheckpoint - Saves best model based on validation metric</li> <li>EarlyStopping - Stops training when metric plateaus</li> <li>LearningRateMonitor - Logs learning rate to W&amp;B</li> </ul>"},{"location":"api-reference/train/#profiling","title":"Profiling","text":"<p>One-time profiling on the first batch (configurable):</p> <pre><code>train:\n  profiling:\n    enabled: true\n    steps: 5\n</code></pre>"},{"location":"api-reference/train/#artifacts","title":"Artifacts","text":"<p>Training automatically logs to W&amp;B:</p> <ul> <li>Training/validation metrics</li> <li>Sample images</li> <li>Training curves</li> <li>Model artifacts with metadata</li> </ul>"},{"location":"api-reference/train/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/train/#hydra-entry-point","title":"Hydra Entry Point","text":"<pre><code># Default training\nuv run python -m ct_scan_mlops.train\n\n# With overrides\nuv run python -m ct_scan_mlops.train model=resnet18 train.max_epochs=50\n</code></pre>"},{"location":"api-reference/train/#invoke-command","title":"Invoke Command","text":"<pre><code>invoke train --args \"model=resnet18\"\n</code></pre>"},{"location":"api-reference/train/#programmatic-training","title":"Programmatic Training","text":"<pre><code>from ct_scan_mlops.train import train_model, LitModel\nfrom ct_scan_mlops.data import ChestCTDataModule\nfrom hydra import compose, initialize\n\nwith initialize(config_path=\"configs\"):\n    cfg = compose(config_name=\"config\")\n\n# Create Lightning model and datamodule\nlit_model = LitModel(cfg)\ndatamodule = ChestCTDataModule(cfg)\n\n# Train\ntrainer = pl.Trainer(max_epochs=10)\ntrainer.fit(lit_model, datamodule=datamodule)\n</code></pre>"},{"location":"api-reference/utils/","title":"Utilities Module","text":"<p>Shared utility functions for the CT Scan MLOps project.</p>"},{"location":"api-reference/utils/#overview","title":"Overview","text":"<p>The utilities module provides common functions used across the project.</p>"},{"location":"api-reference/utils/#functions","title":"Functions","text":""},{"location":"api-reference/utils/#ct_scan_mlops.utils.get_device","title":"get_device","text":"<pre><code>get_device() -&gt; torch.device\n</code></pre> <p>Get the best available device.</p> <p>Priority: CUDA &gt; MPS (Apple Silicon) &gt; CPU</p> Source code in <code>src/ct_scan_mlops/utils.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"Get the best available device.\n\n    Priority: CUDA &gt; MPS (Apple Silicon) &gt; CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    return torch.device(\"cpu\")\n</code></pre>"},{"location":"api-reference/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/utils/#get-best-available-device","title":"Get Best Available Device","text":"<pre><code>from ct_scan_mlops.utils import get_device\n\ndevice = get_device()\nprint(f\"Using device: {device}\")\n# Using device: cuda  (if GPU available)\n# Using device: mps   (if Apple Silicon)\n# Using device: cpu   (fallback)\n</code></pre>"},{"location":"api-reference/utils/#use-with-models","title":"Use with Models","text":"<pre><code>from ct_scan_mlops.utils import get_device\nfrom ct_scan_mlops.model import build_model\n\ndevice = get_device()\nmodel = build_model(cfg).to(device)\n</code></pre>"},{"location":"api-reference/utils/#device-priority","title":"Device Priority","text":"<p>The <code>get_device()</code> function checks for available devices in this order:</p> <ol> <li>CUDA - NVIDIA GPU with CUDA support</li> <li>MPS - Apple Silicon GPU (Metal Performance Shaders)</li> <li>CPU - Fallback to CPU computation</li> </ol> <p>This ensures the best available hardware is used automatically without manual configuration.</p>"},{"location":"development/","title":"Development","text":"<p>Guides for contributing to and understanding the CT Scan MLOps project.</p>"},{"location":"development/#overview","title":"Overview","text":"Guide Description Collaboration W&amp;B team workflow and experiment tracking Project Structure Complete repository structure index"},{"location":"development/#contributing","title":"Contributing","text":""},{"location":"development/#workflow","title":"Workflow","text":"<ol> <li>Pull latest changes:</li> </ol> <pre><code>git pull\ndvc pull  # If data changed\n</code></pre> <ol> <li>Create a branch:</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li>Make changes and test:</li> </ol> <pre><code>invoke ruff    # Format code\ninvoke test    # Run tests\n</code></pre> <ol> <li>Commit and push:</li> </ol> <pre><code>git add .\ngit commit -m \"Add your feature\"\ngit push -u origin feature/your-feature-name\n</code></pre> <ol> <li>Create Pull Request on GitHub</li> </ol>"},{"location":"development/#code-quality","title":"Code Quality","text":""},{"location":"development/#linting-and-formatting","title":"Linting and Formatting","text":"<p>The project uses Ruff for linting and formatting:</p> <pre><code># Run linter + formatter\ninvoke ruff\n\n# Auto-fix issues\ninvoke lint --fix\n</code></pre>"},{"location":"development/#testing","title":"Testing","text":"<p>Tests are written with pytest:</p> <pre><code># Run all tests\ninvoke test\n\n# Run with coverage\npytest --cov=ct_scan_mlops tests/\n</code></pre>"},{"location":"development/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks run automatically on each commit:</p> <pre><code># Install hooks\npre-commit install\n\n# Run manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/#technology-stack","title":"Technology Stack","text":"Component Technology Language Python 3.12 ML Framework PyTorch + PyTorch Lightning Config Hydra Experiment Tracking Weights &amp; Biases Data Versioning DVC Package Manager uv Task Runner invoke Linting Ruff Testing pytest API FastAPI Containerization Docker"},{"location":"development/collaboration/","title":"Team Collaboration","text":"<p>Guide to using Weights &amp; Biases for team experiment tracking.</p>"},{"location":"development/collaboration/#wb-team-setup","title":"W&amp;B Team Setup","text":"<p>This project uses Weights &amp; Biases for experiment tracking with team collaboration.</p>"},{"location":"development/collaboration/#team-information","title":"Team Information","text":"<ul> <li>Team Name: <code>mathiashl-danmarks-tekniske-universitet-dtu</code></li> <li>Project: <code>CT_Scan_MLOps</code></li> <li>Dashboard: W&amp;B Dashboard</li> </ul>"},{"location":"development/collaboration/#getting-started","title":"Getting Started","text":""},{"location":"development/collaboration/#1-accept-team-invitation","title":"1. Accept Team Invitation","text":"<p>You should receive an email invitation to join the W&amp;B team. Click the link to accept.</p> <p>Alternatively:</p> <ol> <li>Go to the team page</li> <li>Request access or contact the team admin</li> </ol>"},{"location":"development/collaboration/#2-verify-team-membership","title":"2. Verify Team Membership","text":"<p>After joining, verify you're in the team:</p> <pre><code>wandb team\n</code></pre> <p>Expected output: <pre><code>mathiashl-danmarks-tekniske-universitet-dtu\n</code></pre></p>"},{"location":"development/collaboration/#3-login-to-wb-first-time-only","title":"3. Login to W&amp;B (First Time Only)","text":"<pre><code>wandb login\n</code></pre> <p>Enter your API key from: https://wandb.ai/authorize</p> <p>One-time setup</p> <p>Your credentials are saved locally. You'll never need to login again on this machine.</p>"},{"location":"development/collaboration/#running-training","title":"Running Training","text":"<p>Once you're a team member, just run training normally:</p> <pre><code># Basic training\ninvoke train\n\n# With custom settings\ninvoke train --args \"train.max_epochs=10 model=resnet18\"\n\n# Quick test (1 epoch)\ninvoke train --args \"train.max_epochs=1 data.batch_size=16\"\n</code></pre> <p>No need to specify <code>--entity</code> - it's already configured.</p>"},{"location":"development/collaboration/#what-gets-logged","title":"What Gets Logged","text":"<p>Every training run automatically logs:</p> What Example Run Author Your W&amp;B username Git Commit Current commit hash Hyperparameters All config values Metrics Loss, accuracy (per batch and epoch) Learning Rate Per epoch Sample Images First batch of first epoch Training Curves Loss/accuracy plots Model Artifacts best_model.pt with metadata System Info GPU, CPU, OS"},{"location":"development/collaboration/#viewing-runs","title":"Viewing Runs","text":""},{"location":"development/collaboration/#team-dashboard","title":"Team Dashboard","text":"<p>https://wandb.ai/mathiashl-danmarks-tekniske-universitet-dtu/CT_Scan_MLOps</p>"},{"location":"development/collaboration/#your-runs","title":"Your Runs","text":"<pre><code>https://wandb.ai/mathiashl-danmarks-tekniske-universitet-dtu/CT_Scan_MLOps?workspace=user-YOUR_USERNAME\n</code></pre>"},{"location":"development/collaboration/#compare-runs","title":"Compare Runs","text":"<ol> <li>Go to project dashboard</li> <li>Select multiple runs (checkboxes)</li> <li>Click \"Compare\"</li> <li>View side-by-side metrics and hyperparameters</li> </ol>"},{"location":"development/collaboration/#best-practices","title":"Best Practices","text":""},{"location":"development/collaboration/#1-tag-your-runs","title":"1. Tag Your Runs","text":"<p>Add descriptive tags for easier filtering:</p> <pre><code>invoke train --args \"wandb.tags=[experiment1,baseline,high-lr]\"\n</code></pre>"},{"location":"development/collaboration/#2-use-descriptive-experiment-names","title":"2. Use Descriptive Experiment Names","text":"<pre><code>invoke train --args \"experiment_name=resnet_pretrained_experiment\"\n</code></pre>"},{"location":"development/collaboration/#3-log-important-notes","title":"3. Log Important Notes","text":"<p>After a run, add notes in W&amp;B:</p> <ol> <li>Open the run in dashboard</li> <li>Click \"Overview\" tab</li> <li>Add notes about what you tried, why, results</li> </ol>"},{"location":"development/collaboration/#4-share-interesting-runs","title":"4. Share Interesting Runs","text":"<p>Copy run URL and share with team: <pre><code>https://wandb.ai/mathiashl-danmarks-tekniske-universitet-dtu/CT_Scan_MLOps/runs/abc123\n</code></pre></p>"},{"location":"development/collaboration/#disable-wb-logging","title":"Disable W&amp;B Logging","text":"<p>For local testing without logging:</p> <pre><code>invoke train --args \"wandb.mode=disabled\"\n</code></pre>"},{"location":"development/collaboration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/collaboration/#permission-denied-error","title":"\"Permission denied\" error","text":"<p>You're not in the team yet.</p> <p>Solution:</p> <ol> <li>Check team membership: <code>wandb team</code></li> <li>Accept invitation email</li> <li>Contact admin if no invitation received</li> </ol>"},{"location":"development/collaboration/#wrong-team-showing","title":"Wrong team showing","text":"<p><code>wandb team</code> shows your personal account.</p> <p>Solution:</p> <pre><code># Relogin and select team\nwandb login --relogin\n\n# Or explicitly set team\nexport WANDB_ENTITY=mathiashl-danmarks-tekniske-universitet-dtu\n</code></pre>"},{"location":"development/collaboration/#useful-commands","title":"Useful Commands","text":"<pre><code># Check team\nwandb team\n\n# Check login status\nwandb status\n\n# View local runs (not synced)\nwandb sync --sync-all\n\n# Pull artifact (model)\nwandb artifact get mathiashl-danmarks-tekniske-universitet-dtu/CT_Scan_MLOps/ct_scan_classifier_model:latest\n\n# Login with different account\nwandb login --relogin\n</code></pre>"},{"location":"development/collaboration/#resources","title":"Resources","text":"<ul> <li>W&amp;B Documentation</li> <li>Team Dashboard</li> <li>GitHub Repository</li> </ul>"},{"location":"development/structure/","title":"Project Structure","text":"<p>Complete index of the CT Scan MLOps repository structure.</p>"},{"location":"development/structure/#root-directory","title":"Root Directory","text":"<pre><code>ct_scan_mlops/\n\u251c\u2500\u2500 pyproject.toml          # Project metadata, dependencies, tool configs\n\u251c\u2500\u2500 tasks.py                # Invoke task definitions (CLI commands)\n\u251c\u2500\u2500 uv.lock                 # Locked dependencies (uv package manager)\n\u251c\u2500\u2500 LICENSE                 # MIT License\n\u2502\n\u251c\u2500\u2500 README.md               # Main project documentation\n\u251c\u2500\u2500 CLAUDE.md               # AI assistant instructions (source of truth)\n\u2502\n\u251c\u2500\u2500 .pre-commit-config.yaml # Pre-commit hook configuration\n\u251c\u2500\u2500 .gitignore              # Git ignore rules\n\u251c\u2500\u2500 .dvcignore              # DVC ignore rules\n\u251c\u2500\u2500 .python-version         # Python version (3.12)\n\u2514\u2500\u2500 .dockerignore           # Docker ignore rules\n</code></pre>"},{"location":"development/structure/#source-code-srcct_scan_mlops","title":"Source Code (<code>src/ct_scan_mlops/</code>)","text":"<p>Main application code for the ML pipeline.</p> <pre><code>src/ct_scan_mlops/\n\u251c\u2500\u2500 __init__.py             # Package initialization\n\u251c\u2500\u2500 model.py                # Neural network architectures (CNN, ResNet18)\n\u251c\u2500\u2500 data.py                 # PyTorch Lightning DataModule\n\u251c\u2500\u2500 dataset.py              # Custom PyTorch Dataset classes\n\u251c\u2500\u2500 train.py                # Training script with Hydra config\n\u251c\u2500\u2500 evaluate.py             # Model evaluation and metrics\n\u251c\u2500\u2500 api.py                  # FastAPI inference endpoint\n\u251c\u2500\u2500 get_data.py             # Data download utilities\n\u251c\u2500\u2500 sweep_train.py          # W&amp;B hyperparameter sweep training\n\u251c\u2500\u2500 sweep_best.py           # Best sweep configuration extraction\n\u251c\u2500\u2500 utils.py                # Shared utility functions\n\u2514\u2500\u2500 visualize.py            # Visualization utilities\n</code></pre>"},{"location":"development/structure/#key-files","title":"Key Files","text":"File Purpose <code>model.py</code> Defines <code>CustomCNN</code> and <code>ResNet18</code> <code>data.py</code> <code>ChestCTDataModule</code> for data loading <code>train.py</code> Hydra-configured training with W&amp;B <code>evaluate.py</code> Evaluation metrics, confusion matrix <code>api.py</code> FastAPI app with <code>/predict</code> endpoint"},{"location":"development/structure/#configuration-configs","title":"Configuration (<code>configs/</code>)","text":"<p>Hydra configuration files.</p> <pre><code>configs/\n\u251c\u2500\u2500 config.yaml             # Main config (imports model, data, train)\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 chest_ct.yaml       # Data paths, batch size, transforms\n\u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 cnn.yaml            # CustomCNN hyperparameters\n\u2502   \u2514\u2500\u2500 resnet18.yaml       # ResNet18 hyperparameters\n\u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 default.yaml        # Default training config\n\u2514\u2500\u2500 sweeps/\n    \u2514\u2500\u2500 train_sweep.yaml    # W&amp;B sweep configuration\n</code></pre>"},{"location":"development/structure/#tests-tests","title":"Tests (<code>tests/</code>)","text":"<p>Test suite using pytest.</p> <pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 conftest.py             # Shared fixtures\n\u251c\u2500\u2500 test_api.py             # API endpoint tests\n\u251c\u2500\u2500 test_config.py          # Configuration validation tests\n\u251c\u2500\u2500 test_data.py            # DataModule and dataset tests\n\u251c\u2500\u2500 test_evaluate.py        # Evaluation function tests\n\u251c\u2500\u2500 test_model.py           # Model architecture tests\n\u2514\u2500\u2500 test_train.py           # Training pipeline tests\n</code></pre>"},{"location":"development/structure/#docker-dockerfiles","title":"Docker (<code>dockerfiles/</code>)","text":"<p>Container definitions.</p> <pre><code>dockerfiles/\n\u251c\u2500\u2500 api.dockerfile          # FastAPI inference server\n\u251c\u2500\u2500 train.dockerfile        # CPU training environment\n\u2514\u2500\u2500 train_cuda.dockerfile   # GPU (CUDA) training environment\n</code></pre>"},{"location":"development/structure/#github-actions-githubworkflows","title":"GitHub Actions (<code>.github/workflows/</code>)","text":"<p>CI/CD pipeline definitions.</p> <pre><code>.github/workflows/\n\u251c\u2500\u2500 cml_data.yaml           # CML data validation\n\u251c\u2500\u2500 docker-build.yaml       # Docker image build\n\u251c\u2500\u2500 docker-publish.yaml     # Docker image publish\n\u251c\u2500\u2500 linting.yaml            # Code linting checks\n\u251c\u2500\u2500 pre-commit.yaml         # Pre-commit hook checks\n\u251c\u2500\u2500 tests.yaml              # Test suite execution\n\u2514\u2500\u2500 deploy_docs.yaml        # Documentation deployment\n</code></pre>"},{"location":"development/structure/#data-models","title":"Data &amp; Models","text":""},{"location":"development/structure/#data-directory-data","title":"Data Directory (<code>data/</code>)","text":"<pre><code>data/\n\u251c\u2500\u2500 raw.dvc                 # DVC tracking for raw data\n\u251c\u2500\u2500 raw/                    # Original dataset (DVC managed)\n\u2502   \u2514\u2500\u2500 chest-ctscan-images/\n\u2514\u2500\u2500 processed/              # Preprocessed tensors\n    \u251c\u2500\u2500 train_images.pt\n    \u251c\u2500\u2500 train_labels.pt\n    \u251c\u2500\u2500 valid_images.pt\n    \u251c\u2500\u2500 valid_labels.pt\n    \u251c\u2500\u2500 test_images.pt\n    \u251c\u2500\u2500 test_labels.pt\n    \u2514\u2500\u2500 stats.pt\n</code></pre>"},{"location":"development/structure/#models-directory-models","title":"Models Directory (<code>models/</code>)","text":"<pre><code>models/\n\u251c\u2500\u2500 .gitkeep                # Placeholder\n\u251c\u2500\u2500 best_model.ckpt         # Lightning checkpoint (after training)\n\u2514\u2500\u2500 model.pt                # PyTorch state dict (for inference)\n</code></pre>"},{"location":"development/structure/#documentation-docs","title":"Documentation (<code>docs/</code>)","text":"<pre><code>docs/\n\u251c\u2500\u2500 mkdocs.yaml             # MkDocs configuration\n\u251c\u2500\u2500 source/                 # Documentation source files\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 api-reference/\n\u2502   \u2514\u2500\u2500 development/\n\u251c\u2500\u2500 GetStarted.md           # Legacy setup guide\n\u251c\u2500\u2500 Structure.md            # This file (legacy)\n\u2514\u2500\u2500 COLLABORATION.md        # W&amp;B guide (legacy)\n</code></pre>"},{"location":"development/structure/#key-entry-points","title":"Key Entry Points","text":"Task File Command Training <code>src/ct_scan_mlops/train.py</code> <code>invoke train</code> Evaluation <code>src/ct_scan_mlops/evaluate.py</code> <code>invoke evaluate</code> API Server <code>src/ct_scan_mlops/api.py</code> <code>invoke api</code> Sweeps <code>src/ct_scan_mlops/sweep_train.py</code> <code>invoke sweep</code> Preprocessing <code>src/ct_scan_mlops/data.py</code> <code>invoke preprocess-data</code>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section covers everything you need to set up and start using the CT Scan MLOps project.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>The CT Scan MLOps project is designed to be easy to set up and use. Follow these guides to get started:</p> <ol> <li>Installation - Set up your development environment</li> <li>Quick Start - Run your first training job</li> </ol>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> Tool Purpose Installation Python 3.12 Runtime python.org uv Package manager See installation guide Git Version control git-scm.com GCP Access Data storage Contact project admin"},{"location":"getting-started/#tldr","title":"TL;DR","text":"<p>For experienced users, here's the minimal setup:</p> <pre><code># Clone and setup\ngit clone https://github.com/SalisMaxima/ct_scan_mlops.git\ncd ct_scan_mlops\nuv venv &amp;&amp; source .venv/bin/activate\nuv sync --all-groups\n\n# Authenticate and get data\ngcloud auth application-default login\ndvc pull\n\n# Train\ninvoke train\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>Quick Start Guide - Essential commands</li> <li>Training Guide - Full training documentation</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Complete setup instructions for the CT Scan MLOps project.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#python-312","title":"Python 3.12","text":"<p>Download and install Python 3.12 from python.org.</p> <p>Verify installation:</p> <pre><code>python --version  # Should show Python 3.12.x\n</code></pre>"},{"location":"getting-started/installation/#uv-package-manager","title":"uv Package Manager","text":"<p>Install uv, the fast Python package manager:</p> <p>=== \"Linux/Mac\"</p> <pre><code>```bash\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n```\n</code></pre> <p>=== \"Windows\"</p> <pre><code>```powershell\npowershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```\n</code></pre>"},{"location":"getting-started/installation/#git","title":"Git","text":"<p>Install Git from git-scm.com.</p>"},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/SalisMaxima/ct_scan_mlops.git\ncd ct_scan_mlops\n</code></pre>"},{"location":"getting-started/installation/#step-2-set-up-environment","title":"Step 2: Set Up Environment","text":"<p>Create a virtual environment and install dependencies:</p> <pre><code># Create virtual environment\nuv venv\n\n# Activate it\nsource .venv/bin/activate  # Linux/Mac\n# .venv\\Scripts\\activate   # Windows\n\n# Install all dependencies\nuv sync --all-groups\n</code></pre>"},{"location":"getting-started/installation/#step-3-authenticate-with-gcp","title":"Step 3: Authenticate with GCP","text":"<p>The dataset is stored in Google Cloud Storage. Authenticate to access it:</p> <pre><code># Login to Google Cloud (one-time setup)\ngcloud auth login\ngcloud auth application-default login\n</code></pre> <p>GCP Access</p> <p>You need access to the GCP project to pull data. Contact the project admin if you don't have access.</p>"},{"location":"getting-started/installation/#step-4-pull-the-dataset","title":"Step 4: Pull the Dataset","text":"<p>Download the chest CT scan dataset from GCS:</p> <pre><code>dvc pull\n</code></pre> <p>This downloads ~120MB of CT scan images to <code>data/raw/</code>.</p>"},{"location":"getting-started/installation/#step-5-verify-setup","title":"Step 5: Verify Setup","text":"<p>Check that everything works:</p> <pre><code># Check Python version\ninvoke python\n\n# Run tests\ninvoke test\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#module-not-found-errors","title":"\"Module not found\" errors","text":"<p>Reinstall all dependencies:</p> <pre><code>uv sync --all-groups\n</code></pre>"},{"location":"getting-started/installation/#dvc-authentication-issues","title":"DVC authentication issues","text":"<p>Re-authenticate with GCP:</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"getting-started/installation/#cudagpu-not-detected","title":"CUDA/GPU not detected","text":"<p>Ensure NVIDIA drivers are installed. For GPU training in Docker:</p> <pre><code>invoke docker-build-cuda\n</code></pre>"},{"location":"getting-started/installation/#windows-specific-issues","title":"Windows-specific issues","text":"<ul> <li>Use PowerShell or Git Bash (not CMD)</li> <li>Replace <code>source .venv/bin/activate</code> with <code>.venv\\Scripts\\activate</code></li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Essential commands</li> <li>Training Guide - Start training models</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Essential commands to get up and running quickly.</p>"},{"location":"getting-started/quickstart/#environment","title":"Environment","text":"<pre><code>invoke bootstrap          # Create new venv and install deps\ninvoke sync               # Sync dependencies\ninvoke dev                # Install with dev dependencies\n</code></pre>"},{"location":"getting-started/quickstart/#data-preprocessing","title":"Data Preprocessing","text":"<p>Before training, preprocess the raw images into tensors for faster loading:</p> <pre><code>invoke preprocess-data    # Preprocess images (run once)\n</code></pre> <p>This creates <code>data/processed/</code> with normalized tensors.</p>"},{"location":"getting-started/quickstart/#training","title":"Training","text":"<pre><code># Train with default CNN\ninvoke train\n\n# Train ResNet18\ninvoke train --args \"model=resnet18\"\n\n# Custom epochs\ninvoke train --args \"train.max_epochs=20\"\n\n# Train without W&amp;B logging\ninvoke train --args \"wandb.mode=disabled\"\n</code></pre>"},{"location":"getting-started/quickstart/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Run W&amp;B Sweeps for hyperparameter optimization:</p> <pre><code># Create a sweep\ninvoke sweep\n\n# Start an agent (use the printed sweep ID)\ninvoke sweep-agent --sweep-id ENTITY/PROJECT/SWEEP_ID\n\n# Get best parameters\ninvoke sweep-best --sweep-id ENTITY/PROJECT/SWEEP_ID\n</code></pre>"},{"location":"getting-started/quickstart/#code-quality","title":"Code Quality","text":"<pre><code>invoke ruff               # Run linter + formatter\ninvoke lint --fix         # Auto-fix linting issues\ninvoke test               # Run tests with coverage\n</code></pre>"},{"location":"getting-started/quickstart/#data-management-dvc","title":"Data Management (DVC)","text":"<pre><code>invoke dvc-pull           # Download data from GCS\ninvoke dvc-push           # Upload data to GCS\n</code></pre>"},{"location":"getting-started/quickstart/#docker","title":"Docker","text":"<pre><code>invoke docker-build       # Build CPU docker images\ninvoke docker-build-cuda  # Build GPU docker image\ninvoke docker-train       # Run training in container\n</code></pre>"},{"location":"getting-started/quickstart/#git-shortcuts","title":"Git Shortcuts","text":"<pre><code>invoke git --message \"Your commit message\"  # Add, commit, push\ninvoke git-status                           # Show git status\n</code></pre>"},{"location":"getting-started/quickstart/#all-commands","title":"All Commands","text":"<p>See all available invoke commands:</p> <pre><code>invoke --list\n</code></pre>"},{"location":"user-guide/","title":"User Guide","text":"<p>Comprehensive guides for using the CT Scan MLOps project.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>This section covers the main workflows for training, evaluating, and deploying CT scan classification models.</p>"},{"location":"user-guide/#guides","title":"Guides","text":"Guide Description Training Train models with different configurations Evaluation Evaluate model performance Configuration Hydra configuration system Docker Containerized training and deployment"},{"location":"user-guide/#typical-workflow","title":"Typical Workflow","text":"<pre><code>graph LR\n    A[Preprocess Data] --&gt; B[Train Model]\n    B --&gt; C[Evaluate]\n    C --&gt; D{Good Results?}\n    D --&gt;|Yes| E[Deploy]\n    D --&gt;|No| F[Tune Hyperparameters]\n    F --&gt; B\n</code></pre> <ol> <li>Preprocess - Convert raw images to tensors</li> <li>Train - Train with your chosen model and hyperparameters</li> <li>Evaluate - Check metrics and confusion matrix</li> <li>Iterate - Tune hyperparameters if needed</li> <li>Deploy - Serve the model via FastAPI</li> </ol>"},{"location":"user-guide/#quick-commands","title":"Quick Commands","text":"<pre><code># Preprocess data (run once)\ninvoke preprocess-data\n\n# Train\ninvoke train\n\n# Evaluate\ninvoke evaluate --checkpoint outputs/.../best_model.ckpt\n\n# Serve API\ninvoke api\n</code></pre>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>Guide to the Hydra configuration system.</p>"},{"location":"user-guide/configuration/#overview","title":"Overview","text":"<p>The project uses Hydra for configuration management. Configs are composable YAML files in the <code>configs/</code> directory.</p>"},{"location":"user-guide/configuration/#config-structure","title":"Config Structure","text":"<pre><code>configs/\n\u251c\u2500\u2500 config.yaml           # Main config (imports model, data, train)\n\u251c\u2500\u2500 data/\n\u2502   \u2514\u2500\u2500 chest_ct.yaml     # Dataset settings\n\u251c\u2500\u2500 model/\n\u2502   \u251c\u2500\u2500 cnn.yaml          # CustomCNN hyperparameters\n\u2502   \u2514\u2500\u2500 resnet18.yaml     # ResNet18 hyperparameters\n\u251c\u2500\u2500 train/\n\u2502   \u2514\u2500\u2500 default.yaml      # Training hyperparameters\n\u2514\u2500\u2500 sweeps/\n    \u2514\u2500\u2500 train_sweep.yaml  # W&amp;B sweep configuration\n</code></pre>"},{"location":"user-guide/configuration/#main-config","title":"Main Config","text":"<p>The main <code>config.yaml</code> composes other configs:</p> <pre><code>defaults:\n  - model: cnn              # or resnet18\n  - data: chest_ct\n  - train: default\n\nexperiment_name: ct_scan_classifier\nseed: 42\n\npaths:\n  data_dir: data/raw\n  output_dir: outputs\n\nwandb:\n  project: CT_Scan_MLOps\n  entity: mathiashl-danmarks-tekniske-universitet-dtu\n</code></pre>"},{"location":"user-guide/configuration/#model-configs","title":"Model Configs","text":""},{"location":"user-guide/configuration/#customcnn-configsmodelcnnyaml","title":"CustomCNN (<code>configs/model/cnn.yaml</code>)","text":"<pre><code>name: custom_cnn\nnum_classes: 4\nhidden_dims: [32, 64, 128, 256]\nfc_hidden: 512\ndropout: 0.3\nbatch_norm: true\n</code></pre>"},{"location":"user-guide/configuration/#resnet18-configsmodelresnet18yaml","title":"ResNet18 (<code>configs/model/resnet18.yaml</code>)","text":"<pre><code>name: resnet18\nnum_classes: 4\npretrained: true\nfreeze_backbone: false\n</code></pre>"},{"location":"user-guide/configuration/#data-config","title":"Data Config","text":"<p><code>configs/data/chest_ct.yaml</code>:</p> <pre><code>batch_size: 32\nnum_workers: 4\nimage_size: 224\nprocessed_path: data/processed\n\nnormalize:\n  mean: [0.485, 0.456, 0.406]\n  std: [0.229, 0.224, 0.225]\n\naugmentation:\n  train:\n    horizontal_flip: true\n    vertical_flip: false\n    rotation_limit: 15\n    brightness_limit: 0.1\n    contrast_limit: 0.1\n</code></pre>"},{"location":"user-guide/configuration/#training-config","title":"Training Config","text":"<p><code>configs/train/default.yaml</code>:</p> <pre><code>max_epochs: 50\ngradient_clip_val: 1.0\n\noptimizer:\n  lr: 0.001\n  weight_decay: 0.0001\n  betas: [0.9, 0.999]\n\nscheduler:\n  eta_min: 0.00001\n\ncheckpoint:\n  monitor: val_acc\n  mode: max\n  save_top_k: 1\n  save_last: true\n\nearly_stopping:\n  enabled: true\n  monitor: val_loss\n  patience: 10\n  mode: min\n</code></pre>"},{"location":"user-guide/configuration/#command-line-overrides","title":"Command Line Overrides","text":"<p>Override any config value from the command line:</p> <pre><code># Single override\ninvoke train --args \"train.max_epochs=100\"\n\n# Multiple overrides\ninvoke train --args \"model=resnet18 train.optimizer.lr=0.0001\"\n\n# Nested overrides\ninvoke train --args \"train.early_stopping.patience=20\"\n\n# List overrides\ninvoke train --args \"model.hidden_dims=[64,128,256,512]\"\n</code></pre>"},{"location":"user-guide/configuration/#multirun-grid-search","title":"Multirun (Grid Search)","text":"<p>Run multiple configurations:</p> <pre><code>uv run python -m ct_scan_mlops.train --multirun \\\n    model=cnn,resnet18 \\\n    train.optimizer.lr=0.001,0.0001\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Some settings can be overridden via environment variables:</p> Variable Purpose Default <code>CONFIG_PATH</code> Config file path <code>configs/config.yaml</code> <code>MODEL_PATH</code> Model checkpoint path <code>models/model.pt</code> <code>WANDB_MODE</code> W&amp;B mode <code>online</code>"},{"location":"user-guide/configuration/#adding-new-configs","title":"Adding New Configs","text":"<ol> <li>Create a new YAML file in the appropriate directory</li> <li>Add it to <code>defaults</code> in <code>config.yaml</code> or reference it from command line</li> </ol> <p>Example: Add a new model config <code>configs/model/efficientnet.yaml</code>:</p> <pre><code>name: efficientnet\nnum_classes: 4\npretrained: true\n</code></pre> <p>Use it:</p> <pre><code>invoke train --args \"model=efficientnet\"\n</code></pre>"},{"location":"user-guide/docker/","title":"Docker","text":"<p>Guide to using Docker for training and deployment.</p>"},{"location":"user-guide/docker/#available-dockerfiles","title":"Available Dockerfiles","text":"Dockerfile Purpose <code>dockerfiles/train.dockerfile</code> CPU training environment <code>dockerfiles/train_cuda.dockerfile</code> GPU training environment <code>dockerfiles/api.dockerfile</code> FastAPI inference server"},{"location":"user-guide/docker/#building-images","title":"Building Images","text":""},{"location":"user-guide/docker/#cpu-training-image","title":"CPU Training Image","text":"<pre><code>invoke docker-build\n</code></pre>"},{"location":"user-guide/docker/#gpu-training-image","title":"GPU Training Image","text":"<pre><code>invoke docker-build-cuda\n</code></pre>"},{"location":"user-guide/docker/#api-image","title":"API Image","text":"<pre><code>docker build -f dockerfiles/api.dockerfile -t ct-scan-api .\n</code></pre>"},{"location":"user-guide/docker/#running-training-in-docker","title":"Running Training in Docker","text":""},{"location":"user-guide/docker/#cpu-training","title":"CPU Training","text":"<pre><code>invoke docker-train\n</code></pre>"},{"location":"user-guide/docker/#gpu-training","title":"GPU Training","text":"<p>Requires NVIDIA Container Toolkit:</p> <pre><code>docker run --gpus all \\\n    -v $(pwd)/data:/app/data \\\n    -v $(pwd)/outputs:/app/outputs \\\n    ct-scan-train-cuda \\\n    invoke train\n</code></pre>"},{"location":"user-guide/docker/#running-the-api","title":"Running the API","text":""},{"location":"user-guide/docker/#build-and-run","title":"Build and Run","text":"<pre><code># Build\ndocker build -f dockerfiles/api.dockerfile -t ct-scan-api .\n\n# Run\ndocker run -p 8000:8000 \\\n    -v $(pwd)/models:/app/models \\\n    -v $(pwd)/configs:/app/configs \\\n    ct-scan-api\n</code></pre>"},{"location":"user-guide/docker/#api-endpoints","title":"API Endpoints","text":"<p>Once running, the API is available at <code>http://localhost:8000</code>:</p> Endpoint Method Description <code>/health</code> GET Health check <code>/predict</code> POST Classify a CT scan image <code>/feedback</code> POST Submit prediction feedback <code>/metrics</code> GET Prometheus metrics"},{"location":"user-guide/docker/#example-request","title":"Example Request","text":"<pre><code>curl -X POST \"http://localhost:8000/predict\" \\\n    -F \"file=@path/to/ct_scan.png\"\n</code></pre> <p>Response:</p> <pre><code>{\n    \"pred_index\": 0,\n    \"pred_class\": \"adenocarcinoma\"\n}\n</code></pre>"},{"location":"user-guide/docker/#environment-variables","title":"Environment Variables","text":"<p>Configure the API container with environment variables:</p> Variable Description Default <code>CONFIG_PATH</code> Path to config file <code>configs/config.yaml</code> <code>MODEL_PATH</code> Path to model weights <code>models/model.pt</code> <code>LOAD_MODEL</code> Load model on startup <code>1</code> <code>FEEDBACK_DIR</code> Directory for feedback images <code>feedback</code> <p>Example:</p> <pre><code>docker run -p 8000:8000 \\\n    -e MODEL_PATH=/app/models/custom_model.pt \\\n    -v $(pwd)/models:/app/models \\\n    ct-scan-api\n</code></pre>"},{"location":"user-guide/docker/#docker-compose","title":"Docker Compose","text":"<p>For development, you can use Docker Compose:</p> <pre><code>version: '3.8'\nservices:\n  api:\n    build:\n      context: .\n      dockerfile: dockerfiles/api.dockerfile\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./models:/app/models\n      - ./configs:/app/configs\n    environment:\n      - MODEL_PATH=/app/models/model.pt\n</code></pre> <p>Run with:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"user-guide/docker/#monitoring","title":"Monitoring","text":"<p>The API exposes Prometheus metrics at <code>/metrics</code>:</p> <ul> <li><code>prediction_error</code>: Counter of prediction errors</li> <li><code>system_cpu_percent</code>: System CPU utilization</li> <li><code>system_memory_percent</code>: System memory utilization</li> <li><code>process_rss_bytes</code>: Process memory usage</li> <li>HTTP request metrics (latency, count, etc.)</li> </ul>"},{"location":"user-guide/evaluation/","title":"Evaluation","text":"<p>Guide to evaluating trained CT scan classification models.</p>"},{"location":"user-guide/evaluation/#basic-evaluation","title":"Basic Evaluation","text":"<p>Evaluate a trained model on the test set:</p> <pre><code>invoke evaluate --checkpoint outputs/.../best_model.ckpt\n</code></pre>"},{"location":"user-guide/evaluation/#checkpoint-formats","title":"Checkpoint Formats","text":"<p>The evaluator supports multiple checkpoint formats:</p> Format Extension Description Lightning <code>.ckpt</code> Full training checkpoint with optimizer state PyTorch <code>.pt</code> Model state dict only (lighter, recommended for deployment)"},{"location":"user-guide/evaluation/#command-options","title":"Command Options","text":"<pre><code># Basic evaluation\ninvoke evaluate --checkpoint path/to/model.ckpt\n\n# With W&amp;B logging\ninvoke evaluate --checkpoint path/to/model.ckpt --wandb --wandb-entity YOUR_USERNAME\n\n# Custom batch size\ninvoke evaluate --checkpoint path/to/model.ckpt --batch-size 64\n\n# Custom output directory\ninvoke evaluate --checkpoint path/to/model.ckpt --output results/\n</code></pre>"},{"location":"user-guide/evaluation/#evaluation-outputs","title":"Evaluation Outputs","text":""},{"location":"user-guide/evaluation/#metrics","title":"Metrics","text":"<p>The evaluator computes and displays:</p> <ul> <li>Test Accuracy: Overall classification accuracy</li> <li>Per-class Precision/Recall/F1: Detailed per-class metrics</li> <li>Macro/Weighted F1: Aggregate F1 scores</li> <li>Confusion Matrix: Visual representation of predictions</li> </ul>"},{"location":"user-guide/evaluation/#classification-report","title":"Classification Report","text":"<pre><code>              precision    recall  f1-score   support\n\nadenocarcinoma       0.95      0.93      0.94        45\nlarge_cell_carcinoma 0.91      0.89      0.90        38\nsquamous_cell_carcinoma 0.88  0.92      0.90        42\n      normal         0.96      0.97      0.96        50\n\n    accuracy                           0.93       175\n   macro avg         0.92      0.93      0.93       175\nweighted avg         0.93      0.93      0.93       175\n</code></pre>"},{"location":"user-guide/evaluation/#confusion-matrix","title":"Confusion Matrix","text":"<p>A confusion matrix is saved to the output directory as <code>confusion_matrix.png</code>.</p>"},{"location":"user-guide/evaluation/#wb-integration","title":"W&amp;B Integration","text":"<p>Log evaluation results to W&amp;B:</p> <pre><code>invoke evaluate \\\n    --checkpoint path/to/model.ckpt \\\n    --wandb \\\n    --wandb-project CT_Scan_MLOps \\\n    --wandb-entity YOUR_USERNAME\n</code></pre> <p>Logged metrics include:</p> <ul> <li>Test accuracy</li> <li>Per-class precision, recall, F1</li> <li>Confusion matrix image</li> </ul>"},{"location":"user-guide/evaluation/#programmatic-evaluation","title":"Programmatic Evaluation","text":"<p>Use the evaluation functions in your own code:</p> <pre><code>from ct_scan_mlops.evaluate import evaluate_model, load_model_from_checkpoint\nfrom ct_scan_mlops.data import create_dataloaders\nfrom ct_scan_mlops.utils import get_device\n\n# Load config and model\nmodel = load_model_from_checkpoint(checkpoint_path, cfg, device)\n\n# Create test dataloader\n_, _, test_loader = create_dataloaders(cfg)\n\n# Evaluate\nmetrics = evaluate_model(\n    model=model,\n    test_loader=test_loader,\n    device=get_device(),\n    save_confusion_matrix=True,\n    output_dir=Path(\"results\"),\n)\n\nprint(f\"Test accuracy: {metrics['test_accuracy']:.4f}\")\n</code></pre>"},{"location":"user-guide/evaluation/#hydra-based-evaluation","title":"Hydra-based Evaluation","text":"<p>For integration with the training pipeline:</p> <pre><code>uv run python -m ct_scan_mlops.evaluate\n</code></pre> <p>This automatically finds the most recent checkpoint and evaluates it.</p>"},{"location":"user-guide/training/","title":"Training","text":"<p>Guide to training CT scan classification models.</p>"},{"location":"user-guide/training/#basic-training","title":"Basic Training","text":"<p>Train with the default configuration (CustomCNN):</p> <pre><code>invoke train\n</code></pre>"},{"location":"user-guide/training/#model-selection","title":"Model Selection","text":"<p>Choose between available models:</p> <pre><code># CustomCNN (default)\ninvoke train\n\n# ResNet18 with transfer learning\ninvoke train --args \"model=resnet18\"\n</code></pre>"},{"location":"user-guide/training/#training-configuration","title":"Training Configuration","text":"<p>Override training parameters from the command line:</p> <pre><code># Custom epochs\ninvoke train --args \"train.max_epochs=50\"\n\n# Custom learning rate\ninvoke train --args \"train.optimizer.lr=0.0001\"\n\n# Multiple overrides\ninvoke train --args \"model=resnet18 train.max_epochs=30 train.optimizer.lr=0.001\"\n</code></pre>"},{"location":"user-guide/training/#wb-experiment-tracking","title":"W&amp;B Experiment Tracking","text":"<p>All training runs are logged to Weights &amp; Biases by default.</p>"},{"location":"user-guide/training/#disable-wb-logging","title":"Disable W&amp;B Logging","text":"<pre><code>invoke train --args \"wandb.mode=disabled\"\n</code></pre>"},{"location":"user-guide/training/#custom-wb-settings","title":"Custom W&amp;B Settings","text":"<pre><code># Custom experiment name\ninvoke train --args \"experiment_name=my_experiment\"\n\n# Add tags\ninvoke train --args \"wandb.tags=[experiment1,baseline]\"\n</code></pre>"},{"location":"user-guide/training/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Use W&amp;B Sweeps for automated hyperparameter optimization.</p>"},{"location":"user-guide/training/#create-and-run-a-sweep","title":"Create and Run a Sweep","text":"<pre><code># 1. Create the sweep\ninvoke sweep\n\n# 2. Start an agent (use the printed sweep ID)\ninvoke sweep-agent --sweep-id ENTITY/PROJECT/SWEEP_ID\n\n# 3. Get the best parameters\ninvoke sweep-best --sweep-id ENTITY/PROJECT/SWEEP_ID\n</code></pre>"},{"location":"user-guide/training/#custom-sweep-configuration","title":"Custom Sweep Configuration","text":"<p>Edit <code>configs/sweeps/train_sweep.yaml</code> to customize the search space:</p> <pre><code>parameters:\n  lr:\n    distribution: log_uniform_values\n    min: 1e-5\n    max: 1e-2\n  batch_size:\n    values: [16, 32, 64]\n</code></pre>"},{"location":"user-guide/training/#training-outputs","title":"Training Outputs","text":"<p>Training outputs are saved to <code>outputs/{experiment_name}/{date}/{time}/</code>:</p> File Description <code>best_model.ckpt</code> Lightning checkpoint with best validation accuracy <code>model.pt</code> PyTorch state dict (for inference) <code>training_curves.png</code> Loss and accuracy plots <code>training.log</code> Detailed training logs <code>.hydra/config.yaml</code> Full configuration used"},{"location":"user-guide/training/#early-stopping","title":"Early Stopping","text":"<p>Early stopping is enabled by default. Configure it in <code>configs/train/default.yaml</code>:</p> <pre><code>early_stopping:\n  enabled: true\n  monitor: val_loss\n  patience: 10\n  mode: min\n</code></pre>"},{"location":"user-guide/training/#gpu-training","title":"GPU Training","text":"<p>GPU is automatically detected and used when available.</p> <pre><code># Check available devices\npython -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}')\"\n</code></pre> <p>For specific GPU configuration:</p> <pre><code>invoke train --args \"train.accelerator=gpu train.devices=1\"\n</code></pre>"},{"location":"user-guide/training/#reproducibility","title":"Reproducibility","text":"<p>Training is reproducible with the same seed:</p> <pre><code>invoke train --args \"seed=42\"\n</code></pre>"},{"location":"user-guide/training/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation Guide - Evaluate trained models</li> <li>Configuration Guide - Full configuration reference</li> </ul>"}]}